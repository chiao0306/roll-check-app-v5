import streamlit as st
import streamlit.components.v1 as components
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeResult
import google.generativeai as genai
from openai import OpenAI
import json
import time
import concurrent.futures
import pandas as pd
from thefuzz import fuzz
from collections import Counter
import re

#å…¨åŸŸç‰¹è¦é…å°ä½¿ç”¨
GLOBAL_FUZZ_THRESHOLD = 80

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="äº¤è²¨å–®ç¨½æ ¸", page_icon="ğŸ­", layout="centered")

# --- CSS æ¨£å¼ ---
st.markdown("""
<style>
/* 1. æ¨™é¡Œå¤§å°æ§åˆ¶ */
h1 {
    font-size: 1.7rem !important; 
    white-space: nowrap !important;
    overflow: hidden !important; 
    text-overflow: ellipsis !important;
}

/* 2. ä¸»åŠŸèƒ½æŒ‰éˆ• (ç´…è‰² Primary) -> è®Šå¤§ã€è®Šé«˜ */
/* é€™æœƒå½±éŸ¿ã€Œé–‹å§‹åˆ†æã€å’Œã€Œç…§ç‰‡æ¸…é™¤ã€ */
button[kind="primary"] {
    height: 60px;               
    font-size: 20px !important; 
    font-weight: bold !important;
    border-radius: 10px !important;
    margin-top: 0px !important;    
    margin-bottom: 5px !important; 
    width: 100%;                
}

/* 3. æ¬¡è¦æŒ‰éˆ• (ç°è‰² Secondary) -> ä¿æŒåŸç‹€ */
/* é€™æœƒå½±éŸ¿æ¯ä¸€å¼µç…§ç‰‡ä¸‹é¢çš„ã€ŒXã€æŒ‰éˆ•ï¼Œè®“å®ƒç¶­æŒå°å°çš„ */
button[kind="secondary"] {
    height: auto !important;
    font-weight: normal !important;
}
</style>
""", unsafe_allow_html=True)
# --- 2. ç§˜å¯†é‡‘é‘°è®€å– ---
try:
    DOC_ENDPOINT = st.secrets["DOC_ENDPOINT"]
    DOC_KEY = st.secrets["DOC_KEY"]
    GEMINI_KEY = st.secrets["GEMINI_KEY"]
    OPENAI_KEY = st.secrets.get("OPENAI_KEY", "")
except:
    st.error("æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹åœ¨ Streamlit Cloud è¨­å®š Secretsã€‚")
    st.stop()

# --- 3. åˆå§‹åŒ– Session State ---
if 'photo_gallery' not in st.session_state: st.session_state.photo_gallery = []
if 'uploader_key' not in st.session_state: st.session_state.uploader_key = 0
if 'auto_start_analysis' not in st.session_state: st.session_state.auto_start_analysis = False

# --- å´é‚Šæ¬„æ¨¡å‹è¨­å®š (åˆä½µç‚ºå–®ä¸€é¸æ“‡) ---
with st.sidebar:
    st.header("æ¨¡å‹è¨­å®š")
    
    # é€™è£¡åŠ å…¥æœ€æ–°çš„ Gemini æ¨¡å‹
    model_options = {
        "Gemini 3 Flash preview": "gemini-3-flash-preview",
        "Gemini 2.5 Flash": "models/gemini-2.5-flash",
        "Gemini 2.5 Flash Lite": "gemini-2.5-flash-lite",
        "Gemini 2.5 Pro": "models/gemini-2.5-pro",
        "GPT-5 Mini": "models/gpt-5-mini-2025-08-07",
        "GPT-5 Nano": "models/gpt-5-nano-2025-08-07",
        
    }
    options_list = list(model_options.keys())
    
    st.subheader("ğŸ¤– ç¸½ç¨½æ ¸ Agent")
    model_selection = st.selectbox(
        "è² è²¬ï¼šè¦æ ¼ã€è£½ç¨‹ã€æ•¸é‡ã€çµ±è¨ˆå…¨åŒ…", 
        options=options_list, 
        index=1, 
        key="main_model"
    )
    main_model_name = model_options[model_selection]
    
    st.divider()
    
    default_auto = st.query_params.get("auto", "true") == "true"
    def update_url_param():
        current_state = "true" if st.session_state.enable_auto_analysis else "false"
        st.query_params["auto"] = current_state

    st.toggle(
        "âš¡ ä¸Šå‚³å¾Œè‡ªå‹•åˆ†æ", 
        value=default_auto, 
        key="enable_auto_analysis", 
        on_change=update_url_param
    )

# --- Excel è¦å‰‡è®€å–å‡½æ•¸ (æœ€çµ‚æ·¨åŒ–ç‰ˆ) ---
@st.cache_data
def get_dynamic_rules(ocr_text, debug_mode=False):
    try:
        import pandas as pd
        from thefuzz import fuzz

        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        ocr_text_clean = str(ocr_text).upper().replace(" ", "").replace("\n", "")
        
        ai_prompt_list = []    # çµ¦ AI çš„
        debug_view_list = []   # çµ¦äººçœ‹çš„

        for index, row in df.iterrows():
            item_name = str(row.get('Item_Name', '')).strip()
            if not item_name or "(é€šç”¨)" in item_name: continue
            
            score = fuzz.partial_ratio(item_name.upper().replace(" ", ""), ocr_text_clean)
            if score >= 85:
                def clean(v): return str(v).strip() if v and str(v) != 'nan' else None
                
                spec = clean(row.get('Standard_Spec', ''))
                f_rename = clean(row.get('Force_Rename', '')) # ğŸ”¥ è®€å–å¼·åˆ¶æ”¹å
                
                u_fr = clean(row.get('Unit_Rule_Freight', ''))
                u_loc = clean(row.get('Unit_Rule_Local', ''))
                u_agg = clean(row.get('Unit_Rule_Agg', ''))

                # --- A. å»ºæ§‹ AI Prompt (åªçµ¦è¦æ ¼) ---
                if not debug_mode:
                    if spec:
                        desc = f"- [åƒè€ƒè³‡è¨Š] {item_name}\n"
                        desc += f"  - æ¨™æº–è¦æ ¼: {spec}\n"
                        ai_prompt_list.append(desc)
                
                # --- B. å»ºæ§‹ Debug é¡¯ç¤º (é‚è¼¯èˆ‡Logicå¾¹åº•è„«é‰¤) ---
                else:
                    block = f"#### â–  {item_name} (åŒ¹é…åº¦ {score}%)\n"
                    
                    block += "**[ AI Prompt è¼¸å…¥ ]**\n"
                    if spec:
                        block += f"- è¦æ ¼æ¨™æº– : `{spec}`\n"
                    else:
                        block += "- (ç„¡ç‰¹å®šè¼¸å…¥)\n"

                    block += "\n**[ Python ç¡¬é‚è¼¯è¨­å®š ]**\n"
                    has_py = False
                    
                    # ğŸ”¥ é€™è£¡é¡¯ç¤º Force_Renameï¼Œçµ•å°æ²’æœ‰ Logic
                    if f_rename:
                        block += f"- âš¡ å¼·åˆ¶æ”¹å : `{f_rename}`\n"
                        has_py = True
                        
                    if u_fr: 
                        block += f"- é‹è²»é‚è¼¯ : `{u_fr}`\n"
                        has_py = True
                    if u_loc:
                        block += f"- å–®é …è¦å‰‡ : `{u_loc}`\n"
                        has_py = True
                    if u_agg:
                        block += f"- èšåˆè¦å‰‡ : `{u_agg}`\n"
                        has_py = True
                    
                    if not has_py:
                        block += "- (ä½¿ç”¨é è¨­é‚è¼¯)\n"
                    
                    block += "\n---\n"
                    debug_view_list.append(block)

        if debug_mode:
            if not debug_view_list: return "ç„¡ç‰¹å®šè¦å‰‡å‘½ä¸­ã€‚"
            return "\n".join(debug_view_list)
        else:
            return "\n".join(ai_prompt_list) if ai_prompt_list else ""

    except Exception as e:
        return f"è®€å–éŒ¯èª¤: {e}"

# --- 4. æ ¸å¿ƒå‡½æ•¸ï¼šAzure ç¥ä¹‹çœ¼ (v2: å¤šé  PDF æ”¯æ´ç‰ˆ) ---
def extract_layout_with_azure(file_obj, endpoint, key):
    client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))
    file_content = file_obj.getvalue()
    
    # åˆ¤æ–·æ˜¯ PDF é‚„æ˜¯åœ–ç‰‡ (MIME type guessing)
    content_type = "application/pdf" if file_content[:4] == b'%PDF' else "application/octet-stream"

    poller = client.begin_analyze_document("prebuilt-layout", file_content, content_type=content_type)
    result: AnalyzeResult = poller.result()
    
    markdown_output = ""
    full_content_list = [] # æ”¹ç”¨ List å­˜æ¯ä¸€é 
    real_page_num = "Unknown"
    
    # å®šç¾©é›œè¨Šé—œéµå­— (ä¿ç•™åŸé‚è¼¯)
    bottom_stop_keywords = ["æ³¨æ„äº‹é …", "ä¸­æ©Ÿå“æª¢å–®ä½", "ä¿å­˜æœŸé™", "è¡¨å–®ç·¨è™Ÿ", "FORM NO", "ç°½ç« "]
    top_right_noise_keywords = [
        "æª¢é©—é¡åˆ¥", "å°ºå¯¸æª¢é©—", "ä¾åœ–é¢æ¨™è¨˜", "ææ–™æª¢é©—", "æˆä»½åˆ†æ", 
        "éç ´å£æ€§", "æ­£å¸¸åŒ–", "é€€ç«", "æ·¬.å›ç«", "è¡¨é¢ç¡¬åŒ–", "è©¦è»Š",
        "æ€§èƒ½æ¸¬è©¦", "è©¦å£“è©¦æ¼", "å‹•.éœå¹³è¡¡è©¦é©—", ":selected:", ":unselected:",
        "æŠ—æ‹‰", "ç¡¬åº¦è©¦é©—", "UT", "PT", "MT"
    ]
    
    # 1. è¡¨æ ¼è™•ç† (Tables) - Azure æœƒè‡ªå‹•æŠ“å‡ºæ‰€æœ‰é é¢çš„è¡¨æ ¼
    if result.tables:
        for idx, table in enumerate(result.tables):
            page_num = table.bounding_regions[0].page_number if table.bounding_regions else "Unknown"
            
            # æ™ºæ…§æ¨™ç±¤åµæ¸¬
            table_tag = "æœªçŸ¥è¡¨æ ¼"
            first_cells = [c.content for c in table.cells if c.row_index == 0]
            first_row_text = "".join(first_cells)
            
            summary_keywords = ["å¯¦äº¤", "ç”³è«‹", "åç¨±åŠè¦ç¯„", "å®Œæˆäº¤è²¨æ—¥æœŸ", "å­˜æ”¾ä½ç½®"]
            detail_keywords = ["è¦ç¯„æ¨™æº–", "æª¢é©—ç´€éŒ„", "å¯¦æ¸¬", "ç·¨è™Ÿ", "å°ºå¯¸", "W3 #", "å…¬å·®"]

            if any(k in first_row_text for k in summary_keywords):
                table_tag = "SUMMARY_TABLE (ç¸½è¡¨)"
            elif any(k in first_row_text for k in detail_keywords):
                table_tag = "DETAIL_TABLE (æ˜ç´°è¡¨)"
            
            markdown_output += f"\n\n=== [{table_tag} | Page {page_num}] ===\n"

            rows = {}
            for cell in table.cells:
                content = cell.content.replace("\n", " ").strip()
                # é€™è£¡ä¸åˆªé™¤ stop keywordsï¼Œå› ç‚ºè¡¨æ ¼é€šå¸¸ä¸æœƒåŒ…å«é å°¾
                
                is_noise = False
                for kw in top_right_noise_keywords:
                    if kw in content:
                        is_noise = True
                        break
                if is_noise: content = "" 

                r, c = cell.row_index, cell.column_index
                if r not in rows: rows[r] = {}
                rows[r][c] = content
            
            for r in sorted(rows.keys()):
                row_cells = []
                if rows[r]:
                    max_col = max(rows[r].keys())
                    for c in range(max_col + 1): 
                        row_cells.append(rows[r].get(c, ""))
                    markdown_output += "| " + " | ".join(row_cells) + " |\n"

    # 2. å…¨æ–‡è™•ç† (Content) - ğŸ”¥ é—œéµä¿®æ”¹ï¼šä¾é é¢åˆ‡å‰²è™•ç† ğŸ”¥
    if result.pages:
        for page in result.pages:
            # é€é spans æŠ“å–è©²é çš„æ–‡å­—ç¯„åœ
            page_text = ""
            for span in page.spans:
                page_text += result.content[span.offset : span.offset + span.length]
            
            # --- é‡å°ã€Œå–®é ã€é€²è¡Œå»é›œè¨Šè™•ç† ---
            
            # A. é ç¢¼æå– (åªæŠ“ç¬¬ä¸€é æˆ–æ¯ä¸€é éƒ½æŠ“)
            if real_page_num == "Unknown":
                match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", page_text, re.IGNORECASE)
                if match: real_page_num = match.group(1)

            # B. é å°¾åˆ‡é™¤ (Bottom Stop) - åªåˆ‡é™¤ã€Œè©²é ã€çš„å°¾å·´
            cut_index = len(page_text)
            for keyword in bottom_stop_keywords:
                idx = page_text.find(keyword)
                if idx != -1 and idx < cut_index:
                    cut_index = idx
            
            clean_page_text = page_text[:cut_index]
            
            # C. å³ä¸Šè§’é›œè¨Šå»é™¤
            for noise in top_right_noise_keywords:
                clean_page_text = clean_page_text.replace(noise, "")
            
            # D. åŠ å…¥è©²é æ–‡å­—åˆ°ç¸½è¡¨ï¼Œä¸¦åŠ ä¸Šæ˜é¡¯çš„åˆ†é æ¨™è¨˜
            full_content_list.append(f"\n--- [PDF Page {page.page_number}] ---\n{clean_page_text}")

    final_full_text = "\n".join(full_content_list)
    header_snippet = final_full_text[:800] if final_full_text else ""

    return markdown_output, header_snippet, final_full_text, None, real_page_num
    
def agent_unified_check(combined_input, full_text_for_search, api_key, model_name):
    import google.generativeai as genai
    import json
    import re
    import time
    
    # 1. æº–å‚™å‹•æ…‹è¦å‰‡
    try:
        dynamic_rules = get_dynamic_rules(full_text_for_search)
    except:
        dynamic_rules = ""

    # 2. å®šç¾© Prompt
    base_prompt = """
    è§’è‰²ï¼šåš´æ ¼çš„æ•¸æ“šæŠ„éŒ„ç¨‹å¼ã€‚é‡å°å–®é è¼¸å…¥ï¼Œä¾æ“š {{RULES_PLACEHOLDER}} åŸ·è¡Œ JSON å¡«ç©ºã€‚
    
    ### 1. æ˜ç´°è¡¨æ•¸æ“š (ä¾†æº: === [DETAIL_TABLE] ===)
    - **item_title**: å®Œæ•´æŠ„éŒ„ï¼Œåš´ç¦éºæ¼ã€Œæœªå†ç”Ÿã€éŠ²è£œã€è»Šä¿®ã€è»¸é ¸ã€ç­‰é—œéµå­—ã€‚
    - **std_spec**: æŠ„éŒ„å« `mm, Â±, +, -` çš„è¦æ ¼æ–‡å­—ã€‚
    - **item_pc_target**: æå–æ¨™é¡Œæœ€å¾Œä¸€å€‹æ‹¬è™Ÿå…§æ•¸å­— (å¦‚ `(4SET)`->`4`), ç„¡å‰‡ `0`ã€‚
    - **batch_total_qty**: è‹¥æ¨™é¡Œå«ã€Œç†±è™•ç†ã€ç ”ç£¨ã€å‹•å¹³è¡¡ã€ï¼Œæå–é¦–æ¬„ç¸½é‡ (å¦‚ `2425KG`)ï¼Œå¦å‰‡ `    - **ds**: æ ¼å¼ `ID:æ•¸å€¼|ID:æ•¸å€¼`ã€‚
      - **è¦å‰‡**: ä¿ç•™å°¾æ•¸0 (å¦‚ `349.90`)ã€‚
      - **é›œè¨Š**: è‹¥å¡—æ”¹/æ¨¡ç³Š/çœ‹ä¸æ¸…ï¼Œæ•¸å€¼å¡« `[!]` (å¦‚ `V1:[!]`)ï¼Œ**åš´ç¦çŒœæ¸¬**ã€‚
    - **category**: å›ºå®šå›å‚³ `null`ã€‚
    
    ### 2. ç¸½è¡¨æ•¸æ“š (ä¾†æº: === [SUMMARY_TABLE] ===)
    - **summary_rows**: æå– `title`, `apply_qty`(ç”³è«‹), `delivery_qty`(å¯¦äº¤), `page`(ç•¶å‰é ç¢¼)ã€‚
    - **header_info**:
      - `job_no`: W/R/O/Y é–‹é ­å·¥ä»¤ã€‚
      - `scheduled_date` / `actual_date`: æ ¼å¼ `YYYY/MM/DD`ã€‚
    
    ### 3. è¼¸å‡ºæ ¼å¼ (JSON Only)
    {
      "header_info": { "job_no": "...", "scheduled_date": "...", "actual_date": "..." },
      "summary_rows": [ { "page": 1, "title": "...", "apply_qty": 0, "delivery_qty": 0 } ],
      "dimension_data": [
         {
           "page": 1, 
           "item_title": "...", 
           "std_spec": "...", 
           "item_pc_target": 0, 
           "batch_total_qty": 0, 
           "category": null, 
           "ds": "ID:å€¼|ID:å€¼" 
         }
      ],
      "issues": []
    }
    """
    
    system_instruction = base_prompt.replace("{{RULES_PLACEHOLDER}}", str(dynamic_rules))

    # 3. è¨­å®š API
    genai.configure(api_key=api_key)
    
    generation_config = {
        "temperature": 0.0,
        "top_p": 0.95,
        "top_k": 40,
        "max_output_tokens": 8192,
        "response_mime_type": "application/json", 
    }

    model = genai.GenerativeModel(
        model_name=model_name,
        generation_config=generation_config,
        system_instruction=system_instruction,
    )

    # 4. åŸ·è¡Œå‘¼å«
    retries = 2
    last_error = None
    
    for attempt in range(retries + 1):
        try:
            response = model.generate_content(combined_input)
            raw_text = response.text.strip()
            final_json = json.loads(raw_text)
            
            # ã€ä¿®æ­£é»ã€‘æ’¿å› Token ä½¿ç”¨é‡ 
            # å¦‚æœä¸åŠ é€™ä¸€æ®µï¼Œä¸»ç¨‹å¼çš„ merge_ai_results å°±æœƒå› ç‚ºæ‰¾ä¸åˆ° "_token_usage" è€Œå¡« 0
            try:
                usage = response.usage_metadata
                final_json["_token_usage"] = {
                    "input": usage.prompt_token_count,
                    "output": usage.candidates_token_count
                }
            except:
                # è¬ä¸€ API æ²’å›å‚³ metadata (æ¥µå°‘è¦‹)ï¼Œçµ¦å€‹é è¨­å€¼
                final_json["_token_usage"] = {"input": 0, "output": 0}
            # ã€ä¿®æ­£çµæŸã€‘
            
            return final_json

        except Exception as e:
            last_error = e
            time.sleep(1)
            continue

    print(f"âŒ AI åˆ†æå¤±æ•—: {last_error}")
    return {
        "header_info": {}, 
        "summary_rows": [], 
        "dimension_data": [], 
        "issues": [{"issue_type": "AI_ERROR", "common_reason": str(last_error)}],
        "_token_usage": {"input": 0, "output": 0} # å¤±æ•—æ™‚ä¹Ÿè¦è£œä¸Šé€™å€‹æ¬„ä½
    }

# --- å¹³è¡Œè™•ç†è¼”åŠ©å‡½å¼ ---

# --- å¼·åˆ¶æ›´åå®˜ (æ­£å¼éœéŸ³ç‰ˆ) ---
def apply_forced_renaming(dimension_data):
    """
    åŠŸèƒ½ï¼šè®€å– Excel å¼·åˆ¶æ”¹åã€‚
    é‚è¼¯ï¼šä½¿ç”¨ã€ŒåŒ…å« (in)ã€é‚è¼¯ï¼Œä¿®æ­£å¤šé¤˜ç¬¦è™Ÿæˆ–æ‹¬è™Ÿå°è‡´çš„åŒ¹é…å¤±æ•—ã€‚
    """
    if not dimension_data: return dimension_data
    import pandas as pd
    
    def clean_key(text):
        t = str(text).upper().replace(" ", "").replace("\n", "").replace("\r", "")
        t = t.replace("ï¼ˆ", "(").replace("ï¼‰", ")")
        return t.strip()

    rename_map = {}
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        
        for i, row in df.iterrows():
            orig = str(row.get('Item_Name', '')).strip()
            target = str(row.get('Force_Rename', '')).strip()
            
            if orig and target and target.lower() != 'nan':
                rename_map[clean_key(orig)] = target
    except:
        pass # æ­£å¼ç‰ˆå®‰éœå¤±æ•—ï¼Œä¸å¹²æ“¾æµç¨‹

    # åŸ·è¡Œæ¯”å°
    for item in dimension_data:
        old_title = item.get('item_title', '')
        ai_clean_key = clean_key(old_title)
        
        # æª¢æŸ¥ Excel çš„ Key æ˜¯å¦åŒ…å«åœ¨ AI çš„æ¨™é¡Œä¸­
        for rule_k, rule_v in rename_map.items():
            if rule_k in ai_clean_key:
                item['item_title'] = rule_v
                item['_original_title'] = old_title
                break 
            
    return dimension_data

# --- ç¾…è³“æ¼¢æ¼”ç®—æ³• (åŠ«å¯Œæ¿Ÿè²§ v1) ---
def rebalance_orphan_data(dimension_data):
    """
    åŠŸèƒ½ï¼šè§£æ±ºã€Œä¸Šä¸€é …çš„å°¾å·´è¢«èª¤åˆ¤çµ¦ä¸‹ä¸€é …ã€çš„å•é¡Œã€‚
    é‚è¼¯ï¼š
    1. éæ­·æ¸…å–®ï¼Œæª¢æŸ¥ç›¸é„°çš„å…©é … (Item A, Item B)ã€‚
    2. å¦‚æœ A çš„æ•¸é‡ < Açš„ç›®æ¨™ (ç¼º) ä¸” B çš„æ•¸é‡ > Bçš„ç›®æ¨™ (å¤š)ã€‚
    3. ä¸” (Bçš„å¤šå‡ºé‡) å¤§ç´„ç­‰æ–¼ (Açš„ç¼ºå£)ã€‚
    4. å°‡ B çš„ã€Œå‰æ®µæ•¸æ“šã€æ¬ç§»çµ¦ A çš„ã€Œå¾Œæ®µã€ã€‚
    """
    if not dimension_data: return dimension_data
    
    # å…ˆåšä¸€å€‹æ·±æ‹·è²ä»¥é˜²è¬ä¸€
    import copy
    data = copy.deepcopy(dimension_data)
    
    # è¼”åŠ©ï¼šè¨ˆç®— ds å­—ä¸²è£¡çš„é …ç›®æ•¸
    def count_ds(ds_str):
        if not ds_str: return 0
        return len([x for x in ds_str.split("|") if ":" in x])

    # è¼”åŠ©ï¼šæ‹†è§£èˆ‡é‡çµ„
    def split_ds(ds_str):
        return [x for x in ds_str.split("|") if ":" in x]
    
    def join_ds(list_data):
        return "|".join(list_data)

    # é–‹å§‹å·¡é‚ (å¾ç¬¬ä¸€é …çœ‹åˆ°å€’æ•¸ç¬¬äºŒé …)
    for i in range(len(data) - 1):
        item_a = data[i]
        item_b = data[i+1]
        
        # 1. å–å¾—ç›®æ¨™å€¼ (Target)
        # æ³¨æ„ï¼šè¦ç¢ºä¿æ‚¨çš„ JSON æ¬„ä½åç¨±æ­£ç¢ºï¼Œé€™è£¡å‡è¨­æ˜¯ 'item_pc_target' æˆ– 'target'
        target_a = int(item_a.get('item_pc_target', 0) or item_a.get('target', 0))
        target_b = int(item_b.get('item_pc_target', 0) or item_b.get('target', 0))
        
        # å¦‚æœæ²’æœ‰ç›®æ¨™å€¼ï¼Œå°±æ²’è¾¦æ³•ç©äº†ï¼Œè·³é
        if target_a == 0 or target_b == 0: continue
        
        # 2. å–å¾—å¯¦éš›å€¼ (Actual String)
        list_a = split_ds(item_a.get('ds', ''))
        list_b = split_ds(item_b.get('ds', ''))
        
        len_a = len(list_a)
        len_b = len(list_b)
        
        # 3. è¨ˆç®—ç¼ºå£èˆ‡ç›ˆé¤˜
        shortage_a = target_a - len_a   # A ç¼ºå¤šå°‘ (ä¾‹å¦‚ 12 - 7 = 5)
        surplus_b = len_b - target_b    # B å¤šå¤šå°‘ (ä¾‹å¦‚ 17 - 12 = 5)
        
        # 4. åˆ¤å®šæ˜¯å¦ç‚ºã€Œèª¤åˆ¤æ¡ˆä¾‹ã€
        # æ¢ä»¶ï¼šA æœ‰ç¼ºï¼ŒB æœ‰å¤šï¼Œä¸” B å¤šå‡ºä¾†çš„é‡å‰›å¥½èƒ½è£œ A (æˆ–ç¨å¾®å¤šä¸€é»é»ä¹Ÿè¡Œ)
        # é€™è£¡è¨­å®šåš´æ ¼ä¸€é»ï¼šB å¤šå‡ºä¾†çš„é‡ >= A ç¼ºçš„é‡
        if shortage_a > 0 and surplus_b >= shortage_a:
            
            # ğŸ”¥ åŸ·è¡Œæ¬ç§»æ‰‹è¡“
            move_count = shortage_a # æ¬ç§»æ•¸é‡ = A ç¼ºçš„æ•¸é‡
            
            # å¾ B çš„é ­éƒ¨åˆ‡ä¸‹ move_count å€‹
            moving_part = list_b[:move_count]
            remaining_b = list_b[move_count:]
            
            # æ¥åˆ° A çš„å°¾éƒ¨
            new_list_a = list_a + moving_part
            
            # 5. æ›´æ–°è³‡æ–™
            item_a['ds'] = join_ds(new_list_a)
            item_b['ds'] = join_ds(remaining_b)
            
            # æ›´æ–°å¾Œè¦åœ¨ Console å°å‡ºç´€éŒ„ (æ–¹ä¾¿é™¤éŒ¯)
            print(f"âš–ï¸ è‡ªå‹•å¹³è¡¡è§¸ç™¼ï¼šå¾ [{item_b.get('item_title')}] ç§»äº† {move_count} ç­†çµ¦ [{item_a.get('item_title')}]")
            
            # æ³¨æ„ï¼šä¸€æ—¦æ¬ç§»éï¼Œç•¶å‰çš„ item_b (ç¾åœ¨è®Šæˆ item_a çš„æ¨£å­äº†) 
            # åœ¨ä¸‹ä¸€æ¬¡è¿´åœˆè®Šæˆ item_a æ™‚ï¼Œè³‡æ–™å·²ç¶“æ˜¯æ­£ç¢ºçš„ï¼Œå¯ä»¥ç¹¼çºŒå¾€ä¸‹æª¢æŸ¥
            
    return data

# --- åˆ‡è›‹ç³•é‚è¼¯ ---
def split_into_batches(pages, max_size=4):
    """
    åˆ‡è›‹ç³•é‚è¼¯ï¼š
    1. å¦‚æœç¸½é æ•¸ <= 4ï¼Œæ•´é¡†æ‹¿å»ã€‚
    2. å¦‚æœ > 4ï¼Œåˆ‡æˆæ•¸å¡Šï¼Œæ¯å¡Šæœ€å¤š 4 é ã€‚
       (ä¾‹å¦‚ 5é  -> [1,2,3,4], [5])
       (ä¾‹å¦‚ 8é  -> [1,2,3,4], [5,6,7,8])
    é€™æ¨£åšæ¯” 3+2 æ›´ç©©ï¼Œå› ç‚ºé€šå¸¸å‰å¹¾é è³‡è¨Šå¯†åº¦æœ€é«˜ã€‚
    """
    for i in range(0, len(pages), max_size):
        yield pages[i:i + max_size]

# --- æ‹¼è›‹ç³•é‚è¼¯ ---
def merge_ai_results(results_list):
    """
    æ‹¼è›‹ç³•é‚è¼¯ï¼šæŠŠä¸¦è¡Œè·‘å›ä¾†çš„ JSON ç¢ç‰‡çµ„åˆæˆä¸€å€‹å®Œæ•´çš„
    """
    final_res = {
        "header_info": {},
        "summary_rows": [],
        "dimension_data": [],
        "issues": [],
        "_token_usage": {"input": 0, "output": 0}
    }
    
    # 1. åˆä½µ Header (é€šå¸¸ç¬¬ä¸€å¡Šæœ€æº–ï¼Œä½†å¦‚æœæœ‰ç¼ºæ¼å¯ä»¥äº’è£œ)
    for res in results_list:
        # ç´¯ç© Token æˆæœ¬
        usage = res.get("_token_usage", {})
        final_res["_token_usage"]["input"] += usage.get("input", 0)
        final_res["_token_usage"]["output"] += usage.get("output", 0)
        
        # ç´¯ç©è³‡æ–™
        final_res["summary_rows"].extend(res.get("summary_rows", []))
        final_res["dimension_data"].extend(res.get("dimension_data", []))
        final_res["issues"].extend(res.get("issues", []))
        
        # Header ç­–ç•¥ï¼šä»¥ç¬¬ä¸€ä»½æœ‰æŠ“åˆ°å·¥ä»¤çš„ç‚ºä¸»
        if not final_res["header_info"].get("job_no"):
            h = res.get("header_info", {})
            if h.get("job_no") and h.get("job_no") != "Unknown":
                final_res["header_info"] = h

    # å†æ¬¡ç¢ºèªï¼šå¦‚æœéƒ½æ²’æŠ“åˆ°ï¼Œè‡³å°‘ä¿ç•™ç¬¬ä¸€ä»½çš„æ—¥æœŸè³‡è¨Š
    if not final_res["header_info"] and results_list:
        final_res["header_info"] = results_list[0].get("header_info", {})

    return final_res

# --- é‡é»ï¼šPython å¼•æ“ ---

def assign_category_by_python(item_title):
    """
    Python åˆ†é¡å®˜ (v71: ä¸‰ä½ä¸€é«”å®Œå…¨ç‰ˆ)
    æ•´åˆå…§å®¹ï¼š
    1. [å¼·åŠ›æ¸…æ´—]: æ”¯æ´å…¨å½¢ç¬¦è™Ÿ (ï¼, Ã—, ï¼‹) è½‰åŠå½¢ï¼Œè§£æ±º OCR è­˜åˆ¥å•é¡Œã€‚
    2. [å†·é…·æ­£å®®]: å°å…¥ v71 é‚è¼¯ï¼Œè‹¥ Excel æœ‰å®Œå…¨åŒ¹é…é …ç›®(å«è¦å‰‡ç‚ºç©ºè€…)ï¼Œçµ•å°ç¦æ­¢æ¨¡ç³ŠåŒ¹é…ã€‚
       - é¿å… "æ­£å®®æ²’å¡«è¦å‰‡ï¼Œå»èª¤æŠ“å°ä¸‰è¦å‰‡" çš„æƒ…æ³ã€‚
    3. [é˜²æš´é£Ÿ]: ä¿ç•™ v2 å»å°¾é‚è¼¯ï¼Œä¿è­· (1SET=4PCS) çµæ§‹ã€‚
    """
    import pandas as pd
    from thefuzz import fuzz
    import re

    # 1. è®€å–å…¨åŸŸé–€æª»
    CURRENT_THRESHOLD = globals().get('GLOBAL_FUZZ_THRESHOLD', 90)

    # ğŸ”¥ [ä¿®æ­£] æ™ºèƒ½å»å°¾å‡½å¼ (v2: é˜²æš´é£Ÿç‰ˆ)
    def remove_tail_info(text):
        # [^\(ï¼ˆ]*? ä»£è¡¨ã€Œæ‹¬è™Ÿå…§å®¹ä¸èƒ½åŒ…å«å…¶ä»–çš„å·¦æ‹¬è™Ÿã€
        return re.sub(r"[\(ï¼ˆ][^\(ï¼ˆ]*?[\)ï¼‰]\s*$", "", str(text)).strip()

    # ğŸ”¥ [å‡ç´š] å¼·åŠ›æ¸…æ´—å‡½å¼ (v36: ç¬¦è™Ÿè½‰åŠå½¢ç‰ˆ)
    def clean_text(text):
        t = str(text).upper() # å¼·åˆ¶å¤§å¯«
        # ç¬¦è™Ÿçµ±ä¸€ (å…¨å½¢è½‰åŠå½¢)
        t = t.replace("ï¼ˆ", "(").replace("ï¼‰", ")")
        t = t.replace("ï¼", "=").replace("ï¼‹", "+").replace("ï¼", "-")
        t = t.replace("Ã—", "X").replace("ï¼Š", "X") # ä¹˜è™Ÿè½‰ X
        t = t.replace("ï¼ƒ", "#").replace("ï¼š", ":")
        # æ¸…é›œè¨Š
        return t.replace(" ", "").replace("\n", "").replace("\r", "").replace('"', '').replace("'", "").strip()

    # ğŸ”¥ [é—œéµæ­¥é©Ÿ] å…ˆåšå»å°¾æ‰‹è¡“ï¼Œå†åšå¼·åŠ›æ¸…ç†
    title_no_tail = remove_tail_info(item_title)
    
    # ç”¨ã€Œå»å°¾+æ¸…æ´—ã€å¾Œçš„ä¹¾æ·¨å­—ä¸²ä¾†åšæ¯”å°éµå€¼ (Phase 2 ç”¨)
    title_clean = clean_text(title_no_tail)
    
    # åŸå§‹å¤§å¯«æª¢æŸ¥ç”¨ (Phase 1 & 3 ç”¨)
    t_upper = str(item_title).upper().replace(" ", "").replace("\n", "").replace('"', "")

    # ==========================================
    # âš¡ï¸ Phase 1: çµ•å°è±å…
    # ==========================================
    if any(k in t_upper for k in ["å‹•å¹³è¡¡", "BALANCING", "ç†±è™•ç†", "HEAT", "TREATING"]):
        return "exempt"

    # ==========================================
    # âš¡ï¸ Phase 2: Excel ç‰¹è¦ (v71 å†·é…·æ­£å®®é‚è¼¯)
    # ==========================================
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        
        best_score = 0
        forced_rule = None
        found_exact = False # ğŸš© æ­£å®®æ——æ¨™

        # 1. å»ºç«‹æœå°‹æ¸…å–® (å…ˆè½‰æˆå­—å…¸ä»¥åˆ©å¿«é€ŸæŸ¥æ‰¾)
        rules_db = {}
        for _, row in df.iterrows():
            iname = str(row.get('Item_Name', '')).strip()
            rule_cat = str(row.get('Category_Rule', '')).strip()
            if rule_cat.lower() == 'nan': rule_cat = "" # è½‰æˆç©ºå­—ä¸²ï¼Œæ–¹ä¾¿å¾ŒçºŒåˆ¤æ–·
            
            if iname:
                # Key å€¼ä¹Ÿè¦ç”¨å¼·åŠ›æ¸…æ´—ç‰ˆ
                key = clean_text(iname)
                rules_db[key] = rule_cat

        # 2. æª¢æŸ¥å®Œå…¨åŒ¹é… (æ­£å®®æª¢æŸ¥)
        if title_clean in rules_db:
            found_exact = True # æ‰¾åˆ°äº†ï¼ç„¡è«–è¦å‰‡æ˜¯ä¸æ˜¯ç©ºçš„ï¼Œéƒ½ç®—æ‰¾åˆ°
            forced_rule = rules_db[title_clean]
            
            # å¦‚æœè¦å‰‡æ˜¯ç©ºçš„ï¼Œä»£è¡¨ User æ•…æ„ç•™ç™½ï¼Œæ„æ€æ˜¯ã€Œä¸è¦ç”¨ç‰¹è¦ï¼Œå›æ­¸ä¸€èˆ¬é‚è¼¯ã€
            # æ­¤æ™‚ forced_rule = ""ï¼Œå¾Œé¢çš„ if forced_rule åˆ¤æ–·æœƒè·³éï¼Œç›´æ¥é€²å…¥ Phase 3
            # é€™æ˜¯æ­£ç¢ºçš„ï¼å› ç‚ºæ‰¾åˆ°äº†æ­£å®®ï¼Œæ‰€ä»¥æˆ‘å€‘ã€Œä¸è·‘æ¨¡ç³ŠåŒ¹é…ã€ï¼Œç›´æ¥å¾€ä¸‹èµ°ã€‚

        # 3. æª¢æŸ¥æ¨¡ç³ŠåŒ¹é… (åªåœ¨æ²’æ‰¾åˆ°æ­£å®®æ™‚åŸ·è¡Œ)
        if not found_exact and rules_db:
            for k, v in rules_db.items():
                if not v: continue # å¦‚æœè¦å‰‡æ˜¯ç©ºçš„ï¼Œæ¨¡ç³ŠåŒ¹é…æŠ“åˆ°ä¹Ÿæ²’ç”¨ï¼Œè·³é
                
                score = fuzz.token_sort_ratio(k, title_clean)
                if score > CURRENT_THRESHOLD: 
                    if score > best_score:
                        best_score = score
                        forced_rule = v
                    elif score == best_score:
                        if len(v) > len(forced_rule if forced_rule else ""):
                            forced_rule = v

        # 4. è§£æè¦å‰‡
        if forced_rule:
            fr = forced_rule.upper()
            if "è±å…" in fr or "EXEMPT" in fr or "SKIP" in fr: return "exempt"
            if "æœ¬é«”" in fr or "UN_REGEN" in fr or "æœªå†ç”Ÿ" in fr: return "un_regen"
            if "å†ç”Ÿ" in fr or "ç²¾è»Š" in fr or "RANGE" in fr: return "range"
            if "éŠ²" in fr or "ç„Š" in fr or "MIN" in fr: return "min_limit"
            if "è»¸é ¸" in fr or "è»¸é ­" in fr or "è»¸ä½" in fr or "MAX" in fr: return "max_limit"
            
    except Exception: pass

    # ==========================================
    # âš¡ï¸ Phase 3: é—œéµå­—è£œåº• (é»ƒé‡‘é †åº)
    # ==========================================
    # èµ°åˆ°é€™è£¡ä»£è¡¨ï¼š
    # 1. Excel è£¡å®Œå…¨æ²’é€™å€‹é …ç›®
    # 2. Excel è£¡æœ‰é€™å€‹é …ç›®(æ­£å®®)ï¼Œä½† Category_Rule æ˜¯ç©ºçš„ -> å›æ­¸ä¸€èˆ¬åˆ¤æ–·

    # 1. [å…§å­”] ç‰¹ä¾‹ï¼šå„ªå…ˆæ¬Šæœ€é«˜ -> range
    if "å…§å­”" in t_upper:
        return "range"

    # 2. [ç„Šè£œ]ï¼šå„ªå…ˆæ–¼è»¸é ¸ -> min_limit
    has_weld = any(k in t_upper for k in ["éŠ²è£œ", "éŠ²æ¥", "ç„Š", "WELD", "é‰€"])
    if has_weld:
        return "min_limit"

    # 3. [æœªå†ç”Ÿ]ï¼šå€åˆ†æœ¬é«”èˆ‡è»¸é ¸
    has_unregen = any(k in t_upper for k in ["æœªå†ç”Ÿ", "UN_REGEN", "ç²—è»Š"])
    if has_unregen:
        if any(k in t_upper for k in ["è»¸é ¸", "è»¸é ­", "è»¸ä½", "JOURNAL"]): 
            return "max_limit"
        return "un_regen"

    # 4. [å†ç”Ÿ/ç²¾åŠ å·¥]ï¼š(ç§»é™¤äº† "è»Šä¿®") -> range
    has_regen = any(k in t_upper for k in ["å†ç”Ÿ", "ç ”ç£¨", "ç²¾åŠ å·¥", "KEYWAY", "GRIND", "MACHIN", "ç²¾è»Š", "çµ„è£", "æ‹†è£", "è£é…", "ASSY", "é…ç£¨"])
    if has_regen:
        return "range"

    return "unknown"

def python_numerical_audit(dimension_data):
    """
    Python å·¥ç¨‹å¼•æ“ (v76: è¦æ ¼å„ªå…ˆæª¢æŸ¥ç‰ˆ)
    é‚è¼¯é †åºä¿®æ­£ï¼š
    1. [åš´æ ¼] è¦æ ¼ç¼ºæ¼æª¢æŸ¥å„ªå…ˆåŸ·è¡Œã€‚å³ä½¿æ˜¯ç†±è™•ç†ï¼Œè‹¥è¦æ ¼æ¬„å…¨ç©ºï¼Œè¦–ç‚ºç•°å¸¸ã€‚
    2. [è±å…] ç¢ºèªæœ‰è¦æ ¼å¾Œï¼Œæ‰åŸ·è¡Œç†±è™•ç†/Exemptçš„è±å…é‚è¼¯ (è·³éæ•¸å­¸æ¯”å°)ã€‚
    3. [é‹ç®—] ä¸€èˆ¬é …ç›®åŸ·è¡Œæ•¸å€¼èˆ‡å…¬å·®æ¯”å°ã€‚
    """
    grouped_errors = {}
    import re
    
    if not dimension_data: return []

    for item in dimension_data:
        ds = str(item.get("ds", ""))
        # è¨»è§£æ‰é€™è¡Œï¼Œç¢ºä¿å³ä½¿æ²’æ•¸æ“šï¼Œä¹Ÿè¦æª¢æŸ¥æœ‰æ²’æœ‰æ¼å¡«è¦æ ¼
        # if not ds: continue  
        
        raw_entries = [p.split(":") for p in ds.split("|") if ":" in p]
        
        # åŸå§‹æ¨™é¡Œè™•ç†
        raw_title = str(item.get("item_title", ""))
        title = raw_title.replace(" ", "").replace('"', "")
        
        # è®€å–åˆ†é¡èˆ‡é‚è¼¯
        cat = str(item.get("category", "")).strip()
        page_num = item.get("page", "?")
        raw_spec = str(item.get("std_spec", "")).replace('"', "")

        # ========================================================
        # ğŸ”¥ [Check 1] è¦æ ¼ç¼ºæ¼æª¢æŸ¥ (å„ªå…ˆæ¬Šæœ€é«˜)
        # ========================================================
        # å³ä½¿æ˜¯ç†±è™•ç†ï¼Œé€™è£¡ä¹Ÿå¿…é ˆéé—œ (å¿…é ˆæœ‰è¦æ ¼å­—ä¸²)
        # å¦‚æœæ¨™é¡Œå­˜åœ¨ï¼Œä½†è¦æ ¼å®Œå…¨æ˜¯ç©ºçš„ -> å ±éŒ¯
        if title and not raw_spec.strip() and len(title) > 1:
            key = (page_num, raw_title, "è¦æ ¼ç¼ºæ¼")
            if key not in grouped_errors:
                grouped_errors[key] = {
                    "page": page_num, 
                    "item": raw_title, 
                    "issue_type": "âš ï¸è¦æ ¼ç¼ºæ¼", 
                    "common_reason": "æœ‰é …ç›®åç¨±ï¼Œä½†æœªåµæ¸¬åˆ°è¦æ ¼æ¨™æº–", 
                    "failures": [{"id": "è¦æ ¼æ¬„", "val": "ç©ºç™½", "calc": "ç¼ºå¤±"}],
                    "source": "ğŸ å·¥ç¨‹å¼•æ“"
                }
            continue # æ—¢ç„¶æ²’è¦æ ¼ï¼Œå¾Œé¢ä¹Ÿä¸ç”¨çœ‹äº†
        # ========================================================

        # ========================================================
        # âš¡ï¸ [Check 2] è±å…é‚è¼¯ (Exemption)
        # ========================================================
        # èµ°åˆ°é€™è£¡ä»£è¡¨ã€Œæœ‰è¦æ ¼ã€äº†ã€‚
        # ç¾åœ¨æª¢æŸ¥æ˜¯å¦ç‚ºã€Œç†±è™•ç†ã€æˆ–ã€Œè±å…é …ç›®ã€ï¼Œå¦‚æœæ˜¯ï¼Œå°±ä¸ç®—å…¬å·®äº†ã€‚
        
        # 1. æ¨™é¡Œé—œéµå­—è±å…
        t_upper = title.upper()
        if any(k in t_upper for k in ["å‹•å¹³è¡¡", "BALANCING", "ç†±è™•ç†", "HEAT"]):
            continue
            
        # 2. åˆ†é¡å®˜æŒ‡ä»¤è±å…
        logic = item.get("sl", {})
        l_type = logic.get("lt", "") 
        
        if "SKIP" in str(l_type).upper() or "EXEMPT" in str(l_type).upper() or "è±å…" in str(l_type):
            continue
        # ========================================================

        # --- ä»¥ä¸‹ç‚ºæ•¸å€¼æå–èˆ‡æª¢æŸ¥é‚è¼¯ (ç¶­æŒä¸è®Š) ---
        
        mm_nums = [float(n) for n in re.findall(r"(\d+\.?\d*)\s*mm", raw_spec)]
        all_nums = [float(n) for n in re.findall(r"(\d+\.?\d*)", raw_spec)]
        noise = [350.0, 300.0, 200.0, 145.0, 130.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0]
        clean_std = [n for n in all_nums if (n in mm_nums) or (n not in noise and n > 5)]

        s_ranges = []
        spec_parts = re.split(r"[\n\r]|[ä¸€äºŒä¸‰å››äº”å…­]|[ï¼ˆ(]\d+[)ï¼‰]|[;ï¼›]", raw_spec)
        
        for part in spec_parts:
            part = part.replace("+-", "Â±").replace("ï¼‹ï¼", "Â±")
            
            if "Â±" in part:
                left_str, right_str = part.split("Â±", 1)
                left_str = left_str.replace(" ", "")
                right_str = right_str.replace(" ", "")
                left_nums = re.findall(r"(\d+\.?\d*)", left_str)
                right_nums = re.findall(r"(\d+\.?\d*)", right_str)
                
                if left_nums and right_nums:
                    b = float(left_nums[-1]) 
                    o = float(right_nums[0])
                    s_ranges.append([round(b - o, 4), round(b + o, 4)])
                    continue 
            
            clean_part = part.replace("mm", "_").replace("MM", "_").replace(" ", "").replace("\n", "").strip()
            if not clean_part: continue
            
            tilde_matches = list(re.finditer(r"(\d+\.?\d*)\s*[_]*\s*[~ï½-]\s*[_]*\s*(\d+\.?\d*)", clean_part))
            has_valid_tilde = False
            if tilde_matches:
                for match in tilde_matches:
                    n1 = float(match.group(1))
                    n2 = float(match.group(2))
                    if abs(n1 - n2) < max(n1, n2) * 0.6:
                        s_ranges.append([round(min(n1, n2), 4), round(max(n1, n2), 4)])
                        has_valid_tilde = True
            if has_valid_tilde: continue

            all_numbers = re.findall(r"[-+]?\d+\.?\d*", clean_part)
            if not all_numbers: continue
            try:
                bases = []
                offsets = []
                for token in all_numbers:
                    val = float(token)
                    if val > 10.0: bases.append(val)
                    elif abs(val) < 10.0: offsets.append(val)
                if bases:
                    for b in bases:
                        if offsets:
                            endpoints = [round(b + o, 4) for o in offsets]
                            if len(endpoints) == 1: endpoints.append(b)
                            s_ranges.append([min(endpoints), max(endpoints)])
                        else:
                            s_ranges.append([b, b])
            except: continue
                    
        if l_type in ["range", "max_limit", "min_limit"]:
            un_regen_target = None
        else:
            s_threshold = logic.get("t", 0)
            un_regen_target = None
            if l_type in ["un_regen", "æœªå†ç”Ÿ"] or ("æœªå†ç”Ÿ" in (cat + title) and not any(k in (cat + title) for k in ["è»¸é ¸", "è»¸é ­", "è»¸ä½"])):
                cands = [n for n in clean_std if n >= 120.0]
                if s_threshold and float(s_threshold) >= 120.0: cands.append(float(s_threshold))
                if cands: un_regen_target = max(cands)

        for entry in raw_entries:
            if len(entry) < 2: continue
            rid = str(entry[0]).strip().replace(" ", "")
            val_raw = str(entry[1]).strip().replace(" ", "")
            
            # ğŸ”¥ [é˜²è­·] M10, N/A, OK é€™äº›éæ•¸å€¼ï¼Œåœ¨é€™è£¡å„ªé›…è·³é (ä¿ç•™å­—ä¸²å­˜åœ¨æ„Ÿ)
            if not val_raw or val_raw.lower() == 'nan': continue
            if val_raw.upper() in ["N/A", "NA", "M10", "OK", "-", ""]: 
                continue 

            try:
                is_passed, reason, t_used, engine_label = True, "", "N/A", "æœªçŸ¥"

                if "[!]" in val_raw:
                    is_passed = False
                    reason = "ğŸ›‘æ•¸æ“šæå£(å£è»Œ)"
                    val_str = "[!]"
                    val = -999.0 
                else:
                    v_m = re.findall(r"\d+\.?\d*", val_raw)
                    val_str = v_m[0] if v_m else val_raw
                    val = float(val_str)

                if val_str != "[!]":
                    is_two_dec = "." in val_str and len(val_str.split(".")[-1]) == 2
                    is_pure_int = "." not in val_str
                else:
                    is_two_dec, is_pure_int = True, True 

                if "min_limit" in str(l_type) or "éŠ²è£œ" in (cat + title):
                    engine_label = "éŠ²è£œ"
                    if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºç´”æ•´æ•¸"
                    elif clean_std:
                        t_used = min(clean_std, key=lambda x: abs(x - val))
                        if val < t_used: is_passed, reason = False, "æ•¸å€¼ä¸è¶³"
                
                elif un_regen_target is not None:
                    engine_label = "æœªå†ç”Ÿ"
                    t_used = un_regen_target
                    if val <= t_used:
                        if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºæ•´æ•¸"
                    elif not is_two_dec: 
                        is_passed, reason = False, "æ‡‰å¡«å…©ä½å°æ•¸"

                elif str(l_type) == "max_limit" or (any(k in (cat + title) for k in ["è»¸é ¸", "è»¸é ­", "è»¸ä½"]) and ("æœªå†ç”Ÿ" in (cat + title))):
                    engine_label = "è»¸é ¸(ä¸Šé™)"
                    candidates = clean_std
                    target = max(candidates) if candidates else 0
                    t_used = target
                    if target > 0:
                        if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºç´”æ•´æ•¸"
                        elif val > target: is_passed, reason = False, f"è¶…éä¸Šé™ {target}"

                elif str(l_type) == "range" or (any(x in (cat + title) for x in ["å†ç”Ÿ", "ç²¾åŠ å·¥", "ç ”ç£¨", "è»Šä¿®", "çµ„è£", "æ‹†è£", "çœŸåœ“åº¦"]) and "æœªå†ç”Ÿ" not in (cat + title)):
                    engine_label = "ç²¾åŠ å·¥"
                    if not is_two_dec:
                        is_passed, reason = False, "æ‡‰å¡«å…©ä½å°æ•¸"
                    elif s_ranges:
                        t_used = str(s_ranges)
                        if not any(r[0] <= val <= r[1] for r in s_ranges): 
                            is_passed, reason = False, "ä¸åœ¨å€é–“å…§"

                if not is_passed:
                    key = (page_num, title, reason)
                    if key not in grouped_errors:
                        grouped_errors[key] = {
                            "page": page_num, "item": title, 
                            "issue_type": f"ç•°å¸¸({engine_label})", 
                            "common_reason": reason, "failures": [],
                            "source": "ğŸ å·¥ç¨‹å¼•æ“"
                        }
                    grouped_errors[key]["failures"].append({"id": rid, "val": val_str, "target": f"åŸºæº–:{t_used}"})
                    
            except: continue
                
    return list(grouped_errors.values())
    
def python_accounting_audit(dimension_data, res_main):
    """
    Python æœƒè¨ˆå®˜ (v71: å†·é…·æ­£å®®ç‰ˆ)
    ä¿®æ­£å…§å®¹ï¼š
    1. [åŒ¹é…é‚è¼¯]: å¼·åˆ¶ã€Œå®Œå…¨åŒ¹é…å„ªå…ˆã€ã€‚
       - å¦‚æœæ‰¾åˆ°å®Œå…¨åŒ¹é…çš„åç¨± (å³ä½¿è¦å‰‡æ¬„ä½æ˜¯ç©ºçš„)ï¼Œç›´æ¥é–å®šè©²è¦å‰‡(æˆ–ç©ºè¦å‰‡)ï¼Œ
       - çµ•å°ç¦æ­¢æ»‘è½åˆ°æ¨¡ç³ŠåŒ¹é…å»ã€Œäº‚èªè¦ªæˆšã€ã€‚
       - é€™è§£æ±ºäº† "æ­£å®®è¦å‰‡ç©ºç™½ï¼Œå»èª¤ç”¨ç›¸ä¼¼ç‰¹è¦çš„å–®ä½è¨­å®š" å°è‡´çš„æœƒè¨ˆç½é›£ã€‚
    2. [åŸºç¤åŠŸèƒ½]: ä¿ç•™ v70 çš„é˜²æš´é£Ÿå»å°¾ã€æ‹¬è™Ÿçµ±ä¸€ã€è»Šä¿®ä¸­ç«‹åŒ–ã€‚
    """
    accounting_issues = []
    from thefuzz import fuzz
    from collections import Counter
    import re
    import pandas as pd 

    # --- 0. è¨­å®š ---
    CURRENT_THRESHOLD = globals().get('GLOBAL_FUZZ_THRESHOLD', 90)

    # æ™ºèƒ½å»å°¾ (v2 é˜²æš´é£Ÿ)
    def remove_tail_info(text):
        return re.sub(r"[\(ï¼ˆ][^\(ï¼ˆ]*?[\)ï¼‰]\s*$", "", str(text)).strip()

    # å¼·åŠ›æ¸…æ´— (v36 åŒ…å«ç¬¦è™Ÿè½‰åŠå½¢)
    def clean_text(text):
        t = str(text).replace("ï¼ˆ", "(").replace("ï¼‰", ")")
        t = t.replace("ï¼", "=").replace("ï¼‹", "+").replace("ï¼", "-") # é †ä¾¿åŠ ä¸Šç¬¦è™Ÿæ”¯æ´
        return t.replace(" ", "").replace("\n", "").replace("\r", "").replace('"', '').replace("'", "").strip()

    def safe_float(value):
        if value is None or str(value).upper() == 'NULL': return 0.0
        if "[!]" in str(value): return "BAD_DATA" 
        cleaned = "".join(re.findall(r"[\d\.]+", str(value).replace(',', '')))
        try: return float(cleaned) if cleaned else 0.0
        except: return 0.0

    def parse_ratio(rule_str):
        if not rule_str or pd.isna(rule_str) or str(rule_str).strip() == "": return 1.0
        match = re.search(r"(\d+)\s*/\s*(\d+)", str(rule_str))
        if match:
            n, d = float(match.group(1)), float(match.group(2))
            if d != 0: return n / d
        try: return float(rule_str)
        except: return 1.0

    # --- 1. è¼‰å…¥è¦å‰‡ ---
    rules_map = {}
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        for _, row in df.iterrows():
            iname = str(row.get('Item_Name', '')).strip()
            if iname: 
                # Key å€¼åšæ¸…æ´—
                key = clean_text(iname)
                # ğŸ”¥ [ä¿®æ­£] å³ä½¿æ¬„ä½æ˜¯ç©ºå€¼ï¼Œä¹Ÿè¦æŠŠ Key å­˜é€²å»ï¼Œä¸¦çµ¦äºˆç©ºå­—å…¸
                # é€™æ¨£æ‰èƒ½åœ¨åŒ¹é…æ™‚çŸ¥é“ã€Œæœ‰é€™å€‹äººã€ï¼Œåªæ˜¯ã€Œæ²’è¦å‰‡ã€
                u_loc = str(row.get('Unit_Rule_Local', ''))
                if u_loc == 'nan': u_loc = ""
                
                u_fr = str(row.get('Unit_Rule_Freight', ''))
                if u_fr == 'nan': u_fr = ""

                u_agg = str(row.get('Unit_Rule_Agg', ''))
                if u_agg == 'nan': u_agg = ""

                rules_map[key] = {
                    "u_local": u_loc,
                    "u_fr": u_fr,
                    "u_agg": u_agg
                }
    except: pass 

    summary_rows = res_main.get("summary_rows", [])
    rule_hits_log = {} 

    # =================================================
    # ğŸ•µï¸â€â™‚ï¸ ç¬¬ä¸€é—œï¼šç¸½è¡¨å…§æˆ°
    # =================================================
    global_sum_tracker = {}
    for s in summary_rows:
        s_title = s.get('title', 'Unknown')
        q_apply = safe_float(s.get('apply_qty', 0))      
        q_deliver = safe_float(s.get('delivery_qty', 0)) 
        if q_deliver == 0 and 'target' in s: q_deliver = safe_float(s.get('target', 0))

        if abs(q_apply - q_deliver) > 0.01:
             accounting_issues.append({
                "page": s.get('page', "ç¸½è¡¨"), 
                "item": f"{s_title}", 
                "issue_type": "ğŸš¨ ç¸½è¡¨æ•¸é‡ç•°å¸¸", 
                "common_reason": f"ç”³è«‹({q_apply}) != å¯¦äº¤({q_deliver})", 
                "failures": [
                    {"é ç¢¼": "ç¸½è¡¨", "é …ç›®åç¨±": "ğŸ“ ç”³è«‹æ•¸é‡", "æ•¸é‡": q_apply, "å‚™è¨»": "åŸå§‹å€¼"},
                    {"é ç¢¼": "ç¸½è¡¨", "é …ç›®åç¨±": "ğŸš› å¯¦äº¤æ•¸é‡", "æ•¸é‡": q_deliver, "å‚™è¨»": "æ ¸å°å€¼"}
                ], 
                "source": "ğŸ æœƒè¨ˆå¼•æ“"
            })
        
        global_sum_tracker[s_title] = {
            "target": q_deliver, 
            "actual": 0, 
            "details": [], 
            "page": s.get('page', "ç¸½è¡¨"),
            "used_mode": "A", 
            "b_reason": ""
        }

    # =================================================
    # ğŸ•µï¸â€â™‚ï¸ ç¬¬äºŒé—œï¼šé€é …æƒæ
    # =================================================
    for item in dimension_data:
        raw_title = item.get("item_title", "")
        
        # æº–å‚™åŒ¹é…ç”¨çš„æ¨™é¡Œ
        title_no_tail = remove_tail_info(raw_title)
        title_clean_rule = clean_text(title_no_tail) # å»å°¾+æ¸…æ´—
        title_clean_full = clean_text(raw_title)     # å®Œæ•´+æ¸…æ´—

        page = item.get("page", "?")
        target_pc = safe_float(item.get("item_pc_target", 0)) 
        batch_qty = safe_float(item.get("batch_total_qty", 0))
        
        # 2.1 è¦å‰‡åŒ¹é… (ğŸ”¥ v71 é‚è¼¯ä¿®æ­£)
        rule_set = None
        matched_rule_name = None
        match_type = ""
        match_score = 0
        found_exact = False

        # A. å®Œå…¨åŒ¹é… (å„ªå…ˆç”¨å»å°¾å¾Œçš„ä¹¾æ·¨å­—ä¸²)
        if title_clean_rule in rules_map:
            rule_set = rules_map[title_clean_rule]
            matched_rule_name = title_clean_rule
            match_type = "å»å°¾å®Œå…¨åŒ¹é…"
            match_score = 100
            found_exact = True # ğŸ”¥ æ¨™è¨˜ï¼šæ‰¾åˆ°äº†æ­£å®®
        
        # B. å®Œæ•´åŒ¹é… (å¦‚æœå»å°¾å¤±æ•—ï¼Œè©¦è©¦çœ‹æ²’å»å°¾çš„)
        if not found_exact and title_clean_full in rules_map:
            rule_set = rules_map[title_clean_full]
            matched_rule_name = title_clean_full
            match_type = "å®Œæ•´å®Œå…¨åŒ¹é…"
            match_score = 100
            found_exact = True # ğŸ”¥ æ¨™è¨˜ï¼šæ‰¾åˆ°äº†æ­£å®®

        # C. æ¨¡ç³ŠåŒ¹é… (ğŸ”¥ åªæœ‰åœ¨ã€Œæ²’æ‰¾åˆ°æ­£å®®ã€æ™‚æ‰åŸ·è¡Œ)
        if not found_exact and rules_map:
            best_score = 0
            best_rule = None
            for k, v in rules_map.items():
                sc = fuzz.token_sort_ratio(k, title_clean_rule) 
                if sc > CURRENT_THRESHOLD and sc > best_score:
                    best_score = sc
                    rule_set = v
                    best_rule = k
            
            if rule_set:
                matched_rule_name = best_rule
                match_type = "æ¨¡ç³ŠåŒ¹é…"
                match_score = best_score
        
        if matched_rule_name:
            if matched_rule_name not in rule_hits_log: rule_hits_log[matched_rule_name] = []
            rule_hits_log[matched_rule_name].append({
                "æ˜ç´°åç¨±": raw_title, "åŒ¹é…é¡å‹": match_type, "åˆ†æ•¸": match_score, "é ç¢¼": page
            })

        # --- ä»¥ä¸‹ç‚ºæ—¢æœ‰é‚è¼¯ ---
        # å¦‚æœ rule_set æ˜¯ç©ºå­—å…¸ (ä»£è¡¨æœ‰æ­£å®®ä½†æ²’è¦å‰‡)ï¼Œé€™è£¡å°±æœƒæ‹¿åˆ°ç©ºå­—ä¸² -> é è¨­ç‚º 1
        u_local = rule_set.get("u_local", "") if rule_set else ""
        u_fr = rule_set.get("u_fr", "") if rule_set else ""
        u_agg = rule_set.get("u_agg", "") if rule_set else ""
        
        ds = str(item.get("ds", ""))
        data_list = [pair.split(":") for pair in ds.split("|") if ":" in pair]
        raw_count = len(data_list) if data_list else 0
        id_counts = Counter([str(e[0]).strip() for e in data_list if len(e)>0])

        # A. å–®é …æª¢æŸ¥
        is_local_exempt = "è±å…" in str(u_local) or "SKIP" in str(u_local).upper() or "EXEMPT" in str(u_local).upper()
        
        # ğŸ”¥ å–®ä½æ›ç®—ï¼šå¦‚æœ rule_set ç‚ºç©ºæˆ– u_local ç‚ºç©ºï¼Œparse_ratio æœƒå›å‚³ 1.0
        ratio = parse_ratio(u_local)
        actual_item_qty = raw_count if batch_qty > 0 else raw_count * ratio
        
        if not is_local_exempt and abs(actual_item_qty - target_pc) > 0.01 and target_pc > 0:
             accounting_issues.append({
                 "page": page, "item": raw_title, "issue_type": "ğŸ›‘ çµ±è¨ˆä¸ç¬¦(å–®é …)", 
                 "common_reason": f"æ¨™é¡Œ {target_pc} != å…§æ–‡ {actual_item_qty} (å€ç‡:{ratio})", 
                 "failures": [], "source": "ğŸ æœƒè¨ˆå¼•æ“"
             })

        # B. é‡è¤‡æª¢æŸ¥ (çœç•¥...)
        journal_family = ["è»¸é ¸", "è»¸é ­", "è»¸ä½", "å…§å­”", "JOURNAL"]
        if "æœ¬é«”" in title_clean_full:
             for rid, count in id_counts.items():
                if count > 1: accounting_issues.append({"page": page, "item": raw_title, "issue_type": "âš ï¸ç·¨è™Ÿé‡è¤‡(æœ¬é«”)", "common_reason": f"{rid} é‡è¤‡ {count}æ¬¡", "failures": []})
        elif any(k in title_clean_full for k in journal_family):
             for rid, count in id_counts.items():
                if count > 2: accounting_issues.append({"page": page, "item": raw_title, "issue_type": "âš ï¸ç·¨è™Ÿé‡è¤‡(è»¸é ¸)", "common_reason": f"{rid} é‡è¤‡ {count}æ¬¡", "failures": []})

        # C. é‹è²» & æ­¸æˆ¶ (çœç•¥...)
        fr_multiplier = parse_ratio(u_fr)
        freight_val = 0.0
        f_note = ""
        u_fr_upper = str(u_fr).upper()
        is_fr_exempt = "è±å…" in u_fr_upper or "SKIP" in u_fr_upper
        is_forced_include = "è¨ˆå…¥" in str(u_fr) or "INCLUDED" in u_fr_upper
        is_default_target = ("æœ¬é«”" in title_clean_full and "æœªå†ç”Ÿ" in title_clean_full) or ("æ–°å“çµ„è£" in title_clean_full)
        
        if not is_fr_exempt and (is_default_target or is_forced_include or fr_multiplier != 1.0):
            freight_val = actual_item_qty * fr_multiplier
            f_note = f"x{fr_multiplier}" if fr_multiplier != 1.0 else ""

        # =================================================
        # Agg Mode (v60: NAN å…ç–«)
        # =================================================
        agg_mode = "B" 
        if u_agg:
            p_clean = str(u_agg).upper().replace(" ", "")
            if p_clean == "NAN": agg_mode = "B"
            elif "EXEMPT" in p_clean or "SKIP" in p_clean: agg_mode = "EXEMPT"
            elif "AB" in p_clean: agg_mode = "AB"
            elif "A" in p_clean: agg_mode = "A"

        agg_multiplier = parse_ratio(u_agg)
        qty_agg = batch_qty if batch_qty > 0 else actual_item_qty * agg_multiplier

        if agg_mode != "EXEMPT":
            for s_title, data in global_sum_tracker.items():
                s_clean = clean_text(s_title)
                
                if (fuzz.partial_ratio("è¼¥è¼ªæ‹†è£.è»Šä¿®æˆ–éŠ²è£œé‹è²»", s_clean) > 70) or ("é‹è²»" in s_clean):
                    if freight_val > 0:
                        data["actual"] += freight_val
                        data["details"].append({"page": page, "title": raw_title, "val": freight_val, "note": f"é‹è²» {f_note}"})
                    continue

                # =========================================================
                # ğŸ§º æ­¥é©Ÿ 1: ç±ƒå­æ’ˆäºº (v70 é‚è¼¯)
                # =========================================================
                s_core = remove_tail_info(s_title) 
                t_core = remove_tail_info(raw_title)
                
                s_core_clean = clean_text(s_core)
                t_core_clean = clean_text(t_core)
                
                score_A = fuzz.token_sort_ratio(s_core_clean, t_core_clean)
                match_A = (score_A >= 90)

                match_B = False
                b_debug_msg = ""
                s_upper_check = s_clean.upper() 

                is_dis = ("ROLLæ‹†è£" in s_upper_check) or ("ROLLçµ„è£" in s_upper_check)
                is_mac = ("ROLLè»Šä¿®" in s_upper_check)
                is_weld = ("ROLLç„Š" in s_upper_check) or ("ROLLé‰€" in s_upper_check) or ("ROLLéŠ²" in s_upper_check)

                has_part_body = "æœ¬é«”" in title_clean_full
                has_part_journal = any(k in title_clean_full for k in journal_family)
                has_act_mac = any(k in title_clean_full for k in ["å†ç”Ÿ", "ç²¾è»Š", "æœªå†ç”Ÿ", "ç²—è»Š"])
                has_act_weld = ("éŠ²è£œ" in title_clean_full or "ç„Š" in title_clean_full or "é‰€" in title_clean_full)
                is_assy = ("çµ„è£" in title_clean_full or "æ‹†è£" in title_clean_full or "æ›´æ›" in title_clean_full)
                
                if is_dis and is_assy: 
                    match_B = True
                    b_debug_msg = "æ‹†è£æ¨¡å¼"
                elif is_mac and (has_part_body or has_part_journal) and has_act_mac: 
                    match_B = True
                    b_debug_msg = "è»Šä¿®æ¨¡å¼"
                elif is_weld and (has_part_body or has_part_journal) and has_act_weld: 
                    match_B = True
                    b_debug_msg = "éŠ²è£œæ¨¡å¼"
                
                if agg_mode == "A": match = match_A
                elif agg_mode == "AB": match = match_A or match_B
                else: match = match_B if match_B else match_A

                # =========================================================
                # ğŸ›‘ æ­¥é©Ÿ 2: æ””æˆªè€… (v69 é‚è¼¯)
                # =========================================================
                if match:
                    t_upper = title_clean_full.upper()
                    
                    s_is_unregen = "æœªå†ç”Ÿ" in s_clean or "ç²—è»Š" in s_clean
                    t_is_unregen = "æœªå†ç”Ÿ" in title_clean_full or "ç²—è»Š" in title_clean_full
                    
                    # ğŸ”¥ v69: è»Šä¿®å·²ç§»é™¤ï¼Œè®Šä¸­ç«‹
                    s_is_regen = ("å†ç”Ÿ" in s_clean or "ç²¾è»Š" in s_clean) and not s_is_unregen
                    t_is_regen = ("å†ç”Ÿ" in title_clean_full or "ç²¾è»Š" in title_clean_full) and not t_is_unregen
                    
                    s_is_weld = ("éŠ²" in s_clean or "ç„Š" in s_clean or "é‰€" in s_clean)
                    t_is_weld = ("éŠ²" in title_clean_full or "ç„Š" in title_clean_full or "é‰€" in title_clean_full)

                    if s_is_unregen and (t_is_regen or t_is_weld): match = False
                    if s_is_regen and (t_is_unregen or t_is_weld): match = False
                    if s_is_weld and (t_is_unregen or t_is_regen): match = False

                    s_is_journal = any(k in s_clean for k in journal_family)
                    t_is_journal = any(k in title_clean_full for k in journal_family) 
                    s_is_body = "æœ¬é«”" in s_clean
                    t_is_body = "æœ¬é«”" in title_clean_full

                    if s_is_body and not s_is_journal and t_is_journal: match = False
                    if s_is_journal and not s_is_body and t_is_body: match = False

                    s_is_heat = "ç†±è™•ç†" in s_clean
                    t_is_heat = "ç†±è™•ç†" in title_clean_full
                    if s_is_heat != t_is_heat: match = False

                    if "TOP" in s_upper_check and "BOTTOM" in t_upper: match = False
                    if "BOTTOM" in s_upper_check and "TOP" in t_upper: match = False

                if match:
                    if match_B and not match_A:
                        data["used_mode"] = "B"
                        data["b_reason"] = b_debug_msg
                    elif match_B and match_A:
                        data["used_mode"] = "AB"

                    data["actual"] += qty_agg
                    c_msg = f"x{agg_multiplier}" if agg_multiplier != 1.0 else ""
                    data["details"].append({"page": page, "title": raw_title, "val": qty_agg, "note": c_msg})

    # =================================================
    # ğŸ•µï¸â€â™‚ï¸ ç¬¬ä¸‰é—œï¼šæ˜ç´°ç¸½çµç®— (Loop 3)
    # =================================================
    for s_title, data in global_sum_tracker.items():
        if abs(data["actual"] - data["target"]) > 0.01: 
            
            mode_label = "Mode A"
            if data["used_mode"] == "B": mode_label = "Mode B ğŸš€"
            elif data["used_mode"] == "AB": mode_label = "Mode A+B"
            
            src_str = f"ğŸ æœƒè¨ˆå¼•æ“ ({mode_label})"

            fail_table = []
            fail_table.append({"é ç¢¼": "ç¸½è¡¨", "é …ç›®åç¨±": f"ğŸ¯ ç›®æ¨™ (å¯¦äº¤)", "æ•¸é‡": data["target"], "å‚™è¨»": "åŸºæº–"})
            for d in data["details"]:
                fail_table.append({"é ç¢¼": f"P.{d['page']}", "é …ç›®åç¨±": d['title'], "æ•¸é‡": d['val'], "å‚™è¨»": d['note']})
            fail_table.append({"é ç¢¼": "âˆ‘", "é …ç›®åç¨±": "åŠ ç¸½çµæœ", "æ•¸é‡": data["actual"], "å‚™è¨»": "ç¸½è¨ˆ"})

            reason_str = f"å¯¦äº¤({data['target']}) != åŠ ç¸½({data['actual']})"
            if data['b_reason']: reason_str += f" | {data['b_reason']}"

            accounting_issues.append({
                "page": data["page"], "item": s_title, 
                "issue_type": "ğŸ›‘ æ˜ç´°åŒ¯ç¸½ä¸ç¬¦", 
                "common_reason": reason_str, 
                "failures": fail_table, 
                "source": src_str
            })

    # ========================================================
    # ğŸ”¥ğŸ”¥ğŸ”¥ã€é€™è£¡æ’å…¥ã€‘æ­¥é©Ÿ 4: æˆç¸¾å–®å›å¯« (Write-Back) ğŸ”¥ğŸ”¥ğŸ”¥
    # ========================================================
    if res_main and "summary_rows" in res_main:
        for row in res_main["summary_rows"]:
            t = row.get('title', '')
            # åªæœ‰ç•¶é€™å€‹é …ç›®æœ‰è¢«è¿½è¹¤åˆ° (global_sum_tracker) æ‰å›å¯«
            if t in global_sum_tracker:
                info = global_sum_tracker[t]
                
                # 1. å›å¯«æ¨¡å¼
                if info['actual'] > 0:
                    row['_audit_mode'] = info['used_mode'] # "A", "B", "AB"
                else:
                    row['_audit_mode'] = "ç„¡åŒ¹é…" # ä»£è¡¨æ ¹æœ¬æ²’ç®—åˆ°åŠå€‹æ˜ç´°

                # 2. å›å¯«åŒ¹é…åˆ°çš„æ˜ç´° (ä¾› UI é¡¯ç¤º)
                # é€™è£¡åªå­˜åç¨±å°±å¥½ï¼ŒUI è‡ªå·±æœƒå»çµ„å­—ä¸²
                matched_names = [d['title'] for d in info['details']]
                row['_audit_details'] = matched_names
                
                # 3. å›å¯«ç‹€æ…‹èˆ‡å‚™è¨»
                row['_audit_status'] = "ğŸ”´ ç•°å¸¸" if abs(info["actual"] - info["target"]) > 0.01 else "ğŸŸ¢ åˆæ ¼"
                row['_audit_note'] = info.get('b_reason', '') # æŠŠ B æ¨¡å¼çš„ç†ç”±å¸¶å‡ºå»

    # ========================================================
    # é€™æ˜¯æ‚¨åŸæœ¬çš„çµå°¾ (HIDDEN_DATA è™•ç†)
    # ========================================================
    if rule_hits_log:
        accounting_issues.append({
            "issue_type": "HIDDEN_DATA",
            "rule_hits": rule_hits_log,
            "fuzz_threshold": CURRENT_THRESHOLD
        })
            
    return accounting_issues
    
def python_process_audit(dimension_data):
    """
    Python æµç¨‹å¼•æ“ (v72.2: æœ€çµ‚å®Œæ•´ç‰ˆ)
    é‚è¼¯æ›´æ–°ï¼š
    1. [è»¸é ¸å°ˆå±¬]: é€£åæ³• (æŸ¥æœ¬é«”) + å…¨é¤åˆ¶ (1,2,3ç¼ºä¸€ä¸å¯)ã€‚
    2. [ä¸€èˆ¬é€šç”¨]: 
       - åŸºç¤æº¯æº: ä¸å¯è·³é—œ (æœ‰3å°±è¦æœ‰1,2)ã€‚
       - ğŸ”¥æ–°å¢è¦å‰‡: æœ‰éŠ²è£œ(2) å‰‡å¿…é ˆæœ‰ å†ç”Ÿ(3)ã€‚(å…è¨±åªåš1ï¼Œä½†è‹¥åšäº†2å°±ä¸€å®šè¦åšå®Œ3)ã€‚
    """
    process_issues = []
    import re
    import pandas as pd
    from thefuzz import fuzz

    # 1. è®€å–å…¨åŸŸé–€æª»
    CURRENT_THRESHOLD = globals().get('GLOBAL_FUZZ_THRESHOLD', 95)

    # è¼”åŠ©å‡½å¼
    def remove_tail_info(text):
        return re.sub(r"[\(ï¼ˆ][^\(ï¼ˆ]*?[\)ï¼‰]\s*$", "", str(text)).strip()

    def clean_text(text):
        t = str(text).upper() 
        t = t.replace("ï¼ˆ", "(").replace("ï¼‰", ")")
        t = t.replace("ï¼", "=").replace("ï¼‹", "+").replace("ï¼", "-")
        t = t.replace("Ã—", "X").replace("ï¼Š", "X") 
        t = t.replace("ï¼ƒ", "#").replace("ï¼š", ":")
        return t.replace(" ", "").replace("\n", "").replace("\r", "").replace('"', '').replace("'", "").strip()

    # 2. è¼‰å…¥è¦å‰‡
    rules_map = {}
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        for _, row in df.iterrows():
            iname = str(row.get('Item_Name', '')).strip()
            p_rule = str(row.get('Process_Rule', '')).strip()
            if p_rule.lower() == 'nan': p_rule = ""
            if iname:
                rules_map[clean_text(iname)] = p_rule.upper()
    except: pass

    # å®šç¾©è£½ç¨‹éšæ®µ
    STAGE_MAP = { 1: "æœªå†ç”Ÿ/ç²—è»Š", 2: "éŠ²è£œ/ç„Šè£œ", 3: "å†ç”Ÿ/ç²¾è»Š", 4: "ç ”ç£¨" }
    history = {} 

    if not dimension_data: return []

    # --- æ­¥é©Ÿ A: è³‡æ–™æ”¶é›† (Parsing) ---
    for item in dimension_data:
        p_num = item.get("page", "?")
        title = str(item.get("item_title", "")).strip()
        
        # æº–å‚™åŒ¹é… Key
        title_no_tail = remove_tail_info(title)
        title_clean_rule = clean_text(title_no_tail)
        ds = str(item.get("ds", ""))
        
        # è±å…
        title_full = clean_text(title)
        if any(k in title_full for k in ["å‹•å¹³è¡¡", "BALANCING", "ç†±è™•ç†", "HEAT"]):
            continue

        # ç‰¹è¦é…å°
        forced_rule = None
        found_exact = False 

        if title_clean_rule in rules_map:
            forced_rule = rules_map[title_clean_rule]
            found_exact = True

        if not found_exact:
            t_no = re.sub(r"[\(ï¼ˆ].*?[\)ï¼‰]", "", title_clean_rule)
            if t_no in rules_map:
                forced_rule = rules_map[t_no]
                found_exact = True

        if not found_exact and rules_map:
            best_score = 0
            for k, v in rules_map.items():
                if not v: continue 
                sc = fuzz.token_sort_ratio(k, title_clean_rule) 
                if sc > CURRENT_THRESHOLD and sc > best_score:
                    best_score = sc
                    forced_rule = v

        # è§£æè»Œé“èˆ‡éšæ®µ
        track = "Unknown"
        stage = 0
        
        if forced_rule:
            fr = forced_rule
            if "è±å…" in fr or "EXEMPT" in fr or "SKIP" in fr: continue 
            
            if "æœ¬é«”" in fr: track = "æœ¬é«”"
            elif "è»¸é ¸" in fr or "è»¸é ­" in fr or "è»¸ä½" in fr: track = "è»¸é ¸"
            
            if "æœªå†ç”Ÿ" in fr or "ç²—è»Š" in fr: stage = 1
            elif "éŠ²" in fr or "ç„Š" in fr or "é‰€" in fr: stage = 2
            elif "å†ç”Ÿ" in fr or "ç²¾è»Š" in fr: stage = 3
            elif "ç ”ç£¨" in fr: stage = 4

        if stage == 0:
            if "ç ”ç£¨" in title_full: stage = 4
            elif any(k in title_full for k in ["éŠ²è£œ", "éŠ²æ¥", "ç„Š", "é‰€"]): stage = 2
            elif "æœªå†ç”Ÿ" in title_full or "ç²—è»Š" in title_full: stage = 1
            elif "å†ç”Ÿ" in title_full or "ç²¾è»Š" in title_full: stage = 3

        if track == "Unknown":
            if "æœ¬é«”" in title_full: track = "æœ¬é«”"
            elif any(k in title_full for k in ["è»¸é ¸", "è»¸é ­", "è»¸ä½", "å…§å­”", "JOURNAL"]): track = "è»¸é ¸"
        
        if track == "Unknown" or stage == 0: continue 

        # æ•¸å€¼æå–
        segments = ds.split("|")
        for seg in segments:
            parts = seg.split(":")
            if len(parts) < 2: continue
            
            rid = parts[0].strip().upper().replace("Ã—", "X").replace("*", "X").replace(" ", "")
            val_str = parts[1].strip()

            nums = re.findall(r"\d+\.?\d*", val_str)
            if not nums: continue
            val = float(nums[0])
            
            key = (rid, track)
            if key not in history: history[key] = {}
            history[key][stage] = {
                "val": val, "page": p_num, "title": title
            }

    # --- æ­¥é©Ÿ B: é å…ˆè¨ˆç®— (é€£åæ³•ç”¨) ---
    body_unregen_ids = set()
    for (rid, track), stages_data in history.items():
        if track == "æœ¬é«”" and 1 in stages_data:
            body_unregen_ids.add(rid)

    # --- æ­¥é©Ÿ C: åŸ·è¡Œç¨½æ ¸ ---
    for (rid, track), stages_data in history.items():
        present_stages = sorted(stages_data.keys())
        if not present_stages: continue
        max_stage = present_stages[-1]
        last_info = stages_data[max_stage]

        # ğŸ”¥ é€šé“ 1: è»¸é ¸ VIP å°ˆå±¬è¦å‰‡
        if track == "è»¸é ¸":
            # 1.1 é€£åæ³•
            if 1 in stages_data:
                if rid not in body_unregen_ids:
                    process_issues.append({
                        "page": stages_data[1]['page'],
                        "item": stages_data[1]['title'],
                        "issue_type": "ğŸ›‘æº¯æºç•°å¸¸(ç¼ºæœ¬é«”)",
                        "common_reason": f"ID [{rid}] æœ‰è»¸é ¸æœªå†ç”Ÿï¼Œå»ç„¡ã€Œæœ¬é«”æœªå†ç”Ÿã€è¨˜éŒ„",
                        "failures": [{"id": rid, "val": "ç¼ºå¤±", "calc": "æœ¬é«”ä¸å­˜åœ¨"}],
                        "source": "ğŸ æµç¨‹å¼•æ“"
                    })

            # 1.2 å…¨é¤åˆ¶ (1,2,3 å¿…å‚™)
            required_set = {1, 2, 3}
            missing_set = required_set - set(stages_data.keys())
            
            if missing_set:
                missing_names = [STAGE_MAP[s] for s in sorted(list(missing_set))]
                process_issues.append({
                    "page": last_info['page'],
                    "item": f"{last_info['title']}",
                    "issue_type": "ğŸ›‘æº¯æºç•°å¸¸(è»¸é ¸ä¸å®Œæ•´)",
                    "common_reason": f"[{track}] å¼·åˆ¶å…¨æµç¨‹ï¼Œç¼ºï¼š{', '.join(missing_names)}",
                    "failures": [{"id": rid, "val": "ç¼ºæ¼", "calc": "æµç¨‹æœªå®Œ"}],
                    "source": "ğŸ æµç¨‹å¼•æ“"
                })
        
        # ğŸ”¥ é€šé“ 2: ä¸€èˆ¬æº¯æº (æœ¬é«”æˆ–å…¶ä»–)
        else:
            # 2.1 åŸºç¤é˜²å‘†ï¼šä¸å¯è·³é—œ (å¾€å›æŸ¥)
            missing_stages = []
            for req_s in range(1, max_stage):
                if req_s not in stages_data: missing_stages.append(STAGE_MAP[req_s])
            
            if missing_stages:
                process_issues.append({
                    "page": last_info['page'],
                    "item": f"{last_info['title']}",
                    "issue_type": "ğŸ›‘æº¯æºç•°å¸¸(ç¼ºæ¼å·¥åº)",
                    "common_reason": f"[{track}] é€²åº¦è‡³ã€{STAGE_MAP[max_stage]}ã€‘ï¼Œç¼ºå‰ç½®ï¼š{', '.join(missing_stages)}",
                    "failures": [{"id": rid, "val": "ç¼ºæ¼", "calc": "å±¥æ­·ä¸å®Œæ•´"}],
                    "source": "ğŸ æµç¨‹å¼•æ“"
                })

            # ğŸ”¥ 2.2 [æ–°å¢] éŠ²è£œå¾ŒåŠç¨‹æª¢æŸ¥ï¼šæœ‰ 2 å‰‡å¿…æœ‰ 3
            # å¦‚æœæœ‰åšéŠ²è£œ (Stage 2)ï¼Œä½†æ²’æœ‰åšå†ç”Ÿ (Stage 3) -> ç•°å¸¸
            if 2 in stages_data and 3 not in stages_data:
                # æ‰¾å‡ºéŠ²è£œé‚£ä¸€é çš„è³‡è¨Šä¾†å ±éŒ¯
                weld_info = stages_data[2]
                process_issues.append({
                    "page": weld_info['page'],
                    "item": f"{weld_info['title']}",
                    "issue_type": "ğŸ›‘æº¯æºç•°å¸¸(è£½ç¨‹æœªå®Œ)",
                    "common_reason": f"[{track}] æœ‰åšéŠ²è£œ(Stage 2)ï¼Œå¾ŒçºŒå¿…é ˆåšå†ç”Ÿ(Stage 3)",
                    "failures": [{"id": rid, "val": "ç¼ºæ¼", "calc": "ç¼ºå†ç”Ÿ"}],
                    "source": "ğŸ æµç¨‹å¼•æ“"
                })

        # --- å°ºå¯¸é‚è¼¯æª¢æŸ¥ ---
        size_rank = { 1: 10, 4: 20, 3: 30, 2: 40 }
        for i in range(len(present_stages)):
            for j in range(i + 1, len(present_stages)):
                s_a = present_stages[i]
                s_b = present_stages[j]
                info_a = stages_data[s_a]
                info_b = stages_data[s_b]
                
                expect_a_smaller = size_rank[s_a] < size_rank[s_b]
                is_violation = False
                if expect_a_smaller:
                    if info_a['val'] >= info_b['val']: is_violation = True
                else:
                    if info_a['val'] <= info_b['val']: is_violation = True
                    
                if is_violation:
                    sign = "<" if expect_a_smaller else ">"
                    process_issues.append({
                        "page": info_b['page'],
                        # ğŸ”¥ ä¿®æ”¹ï¼šç›´æ¥ä½¿ç”¨è©²é …ç›®çš„çœŸå¯¦åç¨±ï¼Œè®“å‰å°èƒ½é…å°äº®ç‡ˆ
                        "item": info_b['title'], 
                        "issue_type": "ğŸ›‘æµç¨‹ç•°å¸¸(å°ºå¯¸å€’ç½®)",
                        "common_reason": f"å°ºå¯¸é‚è¼¯éŒ¯èª¤ï¼š{STAGE_MAP[s_a]} æ‡‰ {sign} {STAGE_MAP[s_b]}",
                        "failures": [{"id": STAGE_MAP[s_a], "val": info_a['val'], "calc": "å‰"}, {"id": STAGE_MAP[s_b], "val": info_b['val'], "calc": "å¾Œ"}],
                        "source": "ğŸ æµç¨‹å¼•æ“"
                    })

    return process_issues
    
def clean_job_no_list(job_list):
    """
    æ¸…æ´—å·¥ä»¤æ¸…å–® (v2: Oç³»åˆ—ç‰¹æ¬Šç‰ˆ)
    é‚è¼¯ï¼š
    1. O é–‹é ­ï¼šåªè¦é•·åº¦å°ï¼Œä¸”è‡³å°‘å« 2 å€‹æ•¸å­— (é¿å…ç´”è‹±æ–‡å–®å­—)ï¼Œå°±æ”¾è¡Œã€‚
    2. W/R/Y é–‹é ­ï¼šå¿…é ˆå«æœ‰ 6 å€‹ä»¥ä¸Šæ•¸å­— (æ“‹æ‰äº‚ç¢¼èˆ‡é›œè¨Š)ã€‚
    3. çµ•å°éæ¿¾ï¼šæ“‹æ‰åŒ…å« "KEY"ã€"WAY" çš„å­—ä¸²ã€‚
    """
    import re
    valid_jobs = []
    seen = set()
    
    for job in job_list:
        j = str(job).strip().upper()
        
        # 1. åŸºæœ¬é–€æª»ï¼šé•·åº¦ 10ï¼ŒæŒ‡å®šé–‹é ­
        if len(j) != 10 or j[0] not in ['W', 'R', 'O', 'Y']:
            continue
            
        # 2. çµ•å°é˜²ç¦¦ï¼šKEYWAY é›œè¨Š
        if "KEY" in j or "WAY" in j:
            continue

        # è¨ˆç®—æ•¸å­—å€‹æ•¸
        digit_count = len(re.findall(r"\d", j))
        
        # 3. åˆ†æµå¯©æŸ¥
        is_valid = False
        
        if j.startswith("O"):
            # ã€Oç³»åˆ—è¦å‰‡ã€‘ï¼šå¯¬é¬†ï¼Œä½†è‡³å°‘è¦æœ‰ 2 å€‹æ•¸å­— (OW62JGGY11 æœ‰4å€‹æ•¸å­— -> PASS)
            # é˜²æ­¢å–®ç´”è¢«èª¤åˆ¤ç‚ºOé–‹é ­çš„è‹±æ–‡å–®å­—
            if digit_count >= 2:
                is_valid = True
        else:
            # ã€W/R/Yç³»åˆ—è¦å‰‡ã€‘ï¼šåš´æ ¼ï¼Œå¿…é ˆæœ‰ 6 å€‹ä»¥ä¸Šæ•¸å­—
            # W363150820 (9å€‹æ•¸å­—) -> PASS
            # YWAYCKEYWA (0å€‹æ•¸å­—) -> FAIL
            # W3BCC350PI (3å€‹æ•¸å­—) -> FAIL (å› ç‚ºæ•¸å­—å¤ªå°‘)
            if digit_count >= 6:
                is_valid = True
                
        if is_valid and j not in seen:
            valid_jobs.append(j)
            seen.add(j)
            
    return valid_jobs
    
def python_header_audit_batch(photo_gallery, ai_res_json):
    """
    Python è¡¨é ­ç¨½æ ¸å®˜ (Batch æ¶æ§‹é©é…ç‰ˆ v31: æ•´åˆå·¥ä»¤æ·¨åŒ–)
    """
    header_issues = []
    import re
    from datetime import datetime

    # --- 1. æ··å–®æª¢æŸ¥ (åˆ©ç”¨ OCR åŸå§‹æ–‡å­—) ---
    # ç­–ç•¥ï¼šç›´æ¥ç”¨ Regex åœ¨æ¯ä¸€é çš„æ–‡å­—è£¡æ’ˆ W/R/O/Y é–‹é ­çš„å­—ä¸²
    job_pattern = r"([WROY][A-Z0-9]{9})" # æŠ“ 10 ç¢¼
    found_jobs_map = {} # { "å·¥ä»¤è™Ÿ": [é ç¢¼list] }

    for idx, item in enumerate(photo_gallery):
        txt = item.get('full_text', '').upper().replace(" ", "").replace("-", "")
        # å°‹æ‰¾æ‰€æœ‰ç–‘ä¼¼å·¥ä»¤çš„å­—ä¸²
        matches = re.findall(job_pattern, txt)
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ [é—œéµä¿®æ”¹] å‘¼å«æ·¨åŒ–å‡½å¼éæ¿¾é›œè¨Š ğŸ”¥ğŸ”¥ğŸ”¥
        valid_matches = clean_job_no_list(matches)
        
        # åªæŠŠã€Œæ·¨åŒ–å¾Œã€çš„å·¥ä»¤åŠ å…¥æ¸…å–®
        for job in valid_matches:
            if job not in found_jobs_map: found_jobs_map[job] = []
            found_jobs_map[job].append(idx + 1)

    # å¦‚æœæ‰¾åˆ°å¤šç¨®ä¸åŒçš„å·¥ä»¤ -> å ±è­¦
    if len(found_jobs_map) > 1:
        details = [f"{k} (P.{v})" for k, v in found_jobs_map.items()]
        header_issues.append({
            "page": "å¤šé ", "item": "å·¥ä»¤å–®è™Ÿ", "issue_type": "ğŸš¨ åš´é‡æ··å–®",
            "common_reason": f"åµæ¸¬åˆ°å¤šç¨®å·¥ä»¤ï¼š{', '.join(details)}",
            "failures": [{"id": "å…§å®¹", "val": str(found_jobs_map)}],
            "source": "ğŸ è¡¨é ­ç¨½æ ¸(OCR)"
        })

    # --- 2. æ ¼å¼èˆ‡æ—¥æœŸæª¢æŸ¥ (åˆ©ç”¨ AI JSON) ---
    h_info = ai_res_json.get("header_info", {})
    
    # å·¥ä»¤æ ¼å¼ (é‡å° AI æœ€çµ‚èªå®šçš„é‚£ä¸€çµ„)
    ai_job = h_info.get("job_no", "Unknown")
    if ai_job and ai_job != "Unknown":
        clean_job = ai_job.upper().replace(" ", "").replace("-", "")
        if not re.match(r"^[WROY][A-Z0-9]{9}$", clean_job):
            header_issues.append({
                "page": "è¡¨é ­", "item": "å·¥ä»¤æ ¼å¼", "issue_type": "âš ï¸ æ ¼å¼éŒ¯èª¤",
                "common_reason": f"AI è­˜åˆ¥å·¥ä»¤ {ai_job} æ ¼å¼ä¸ç¬¦ (éœ€10ç¢¼ï¼ŒW/R/O/Yé–‹é ­)",
                "failures": [{"id": "è­˜åˆ¥å€¼", "val": ai_job}],
                "source": "ğŸ è¡¨é ­ç¨½æ ¸(AI)"
            })

    # æ—¥æœŸé‚è¼¯ (å¯¦éš› <= é å®š)
    d_sch = h_info.get("scheduled_date", "Unknown")
    d_act = h_info.get("actual_date", "Unknown")
    
    if d_sch != "Unknown" and d_act != "Unknown":
        try:
            # å˜—è©¦è§£æ YYYY/MM/DD
            dt_sch = datetime.strptime(d_sch.replace("-", "/"), "%Y/%m/%d")
            dt_act = datetime.strptime(d_act.replace("-", "/"), "%Y/%m/%d")
            
            if dt_act > dt_sch:
                 header_issues.append({
                    "page": "è¡¨é ­", "item": "äº¤è²¨æ™‚æ•ˆ", "issue_type": "â° é€¾æœŸäº¤è²¨",
                    "common_reason": f"å¯¦éš› {d_act} æ™šæ–¼ é å®š {d_sch}",
                    "failures": [{"id": "å»¶é²å¤©æ•¸", "val": f"{(dt_act - dt_sch).days} å¤©"}], 
                    "source": "ğŸ è¡¨é ­ç¨½æ ¸(AI)"
                })
        except:
            pass # æ—¥æœŸæ ¼å¼è®€ä¸æ‡‚ï¼Œè·³é

    return header_issues
    
def consolidate_issues(issues):
    """
    ğŸ—‚ï¸ ç•°å¸¸åˆä½µå™¨ï¼šå°‡ã€Œé …ç›®ã€ã€ã€ŒéŒ¯èª¤é¡å‹ã€ã€ã€ŒåŸå› ã€å®Œå…¨ç›¸åŒçš„ç•°å¸¸åˆä½µæˆä¸€å¼µå¡ç‰‡
    """
    grouped = {}
    for i in issues:
        key = (i.get('item', ''), i.get('issue_type', ''), i.get('common_reason', ''))
        if key not in grouped:
            grouped[key] = i.copy()
            grouped[key]['pages_set'] = {str(i.get('page', '?'))}
            grouped[key]['failures'] = i.get('failures', []).copy()
        else:
            grouped[key]['pages_set'].add(str(i.get('page', '?')))
            grouped[key]['failures'].extend(i.get('failures', []))
            
    result = []
    for key, val in grouped.items():
        sorted_pages = sorted(list(val['pages_set']), key=lambda x: int(x) if x.isdigit() else 999)
        val['page'] = ", ".join(sorted_pages)
        del val['pages_set']
        result.append(val)
    return result
    
# --- 6. æ‰‹æ©Ÿç‰ˆ UI èˆ‡ æ ¸å¿ƒåŸ·è¡Œé‚è¼¯ ---
st.title("ğŸ­ äº¤è²¨å–®ç¨½æ ¸")

data_source = st.radio(
    "è«‹é¸æ“‡è³‡æ–™ä¾†æºï¼š", 
    ["ğŸ“¸ ä¸Šå‚³ç…§ç‰‡", "ğŸ“‚ ä¸Šå‚³ JSON æª”", "ğŸ“Š ä¸Šå‚³ Excel æª”"], 
    horizontal=True
)

with st.container(border=True):
    # --- æƒ…æ³ A: ä¸Šå‚³ç…§ç‰‡ ---
    if data_source == "ğŸ“¸ ä¸Šå‚³ç…§ç‰‡":
        if st.session_state.get('source_mode') == 'json' or st.session_state.get('source_mode') == 'excel':
            st.session_state.photo_gallery = []
            st.session_state.source_mode = 'image'

        uploaded_files = st.file_uploader(
            "è«‹é¸æ“‡ JPG/PNG/PDF ç…§ç‰‡...", 
            type=['jpg', 'png', 'jpeg', 'pdf'], 
            accept_multiple_files=True, 
            key=f"uploader_{st.session_state.uploader_key}"
        )
        
        if uploaded_files:
            for f in uploaded_files: 
                if not any(x['file'].name == f.name for x in st.session_state.photo_gallery if x['file']):
                    st.session_state.photo_gallery.append({
                        'file': f, 
                        'table_md': None, 
                        'header_text': None,
                        'full_text': None,
                        'raw_json': None
                    })
            st.session_state.uploader_key += 1
            if st.session_state.enable_auto_analysis:
                st.session_state.auto_start_analysis = True
            components.html("""<script>window.parent.document.body.scrollTo(0, window.parent.document.body.scrollHeight);</script>""", height=0)
            st.rerun()

    # --- æƒ…æ³ B: ä¸Šå‚³ JSON ---
    elif data_source == "ğŸ“‚ ä¸Šå‚³ JSON æª”":
        st.info("ğŸ’¡ è«‹é»æ“Šä¸‹æ–¹æŒ‰éˆ•ï¼Œå¾ä½ çš„è³‡æ–™å¤¾é¸æ“‡ä¹‹å‰ä¸‹è¼‰çš„ `.json` æª”ã€‚")
        uploaded_json = st.file_uploader("ä¸Šå‚³JSONæª”", type=['json'], key="json_uploader")
        
        if uploaded_json:
            try:
                current_file_name = uploaded_json.name
                if st.session_state.get('last_loaded_json_name') != current_file_name:
                    json_data = json.load(uploaded_json)
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'json'
                    st.session_state.last_loaded_json_name = current_file_name
                    
                    import re
                    for page in json_data:
                        real_page = "Unknown"
                        full_text = page.get('full_text', '')
                        if full_text:
                            match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", full_text, re.IGNORECASE)
                            if match:
                                real_page = match.group(1)
                        
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': page.get('table_md'),
                            'header_text': page.get('header_text'),
                            'full_text': full_text,
                            'raw_json': page.get('raw_json'),
                            'real_page': real_page
                        })
                    
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ JSON: {current_file_name}", icon="ğŸ“‚")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“‚ ç›®å‰è¼‰å…¥ JSONï¼š**{uploaded_json.name}**")
            except Exception as e:
                st.error(f"JSON æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e}")

    # --- æƒ…æ³ C: ä¸Šå‚³ Excel (ç´”ä»£ç¢¼ç›´è®€ç‰ˆ - ä¸ç¶“ AI) ---
    elif data_source == "ğŸ“Š ä¸Šå‚³ Excel æª”":
        st.info("ğŸ’¡ ä½¿ç”¨ã€Œç´”ä»£ç¢¼ç›´è®€ã€æ¨¡å¼ï¼šç›´æ¥æå– Excel æ•¸å€¼ï¼Œé€Ÿåº¦æœ€å¿«ä¸”æº–ç¢ºã€‚")
        uploaded_xlsx = st.file_uploader("ä¸Šå‚³ Excel æª”", type=['xlsx', 'xls', 'xlsm'], key="xlsx_uploader")
        
        if uploaded_xlsx:
            try:
                current_file_name = uploaded_xlsx.name
                if st.session_state.get('last_loaded_xlsx_name') != current_file_name:
                    
                    # 1. è®€å– Excel (è®€å–æ‰€æœ‰å…§å®¹ç‚ºå­—ä¸²ï¼Œé¿å… 001 è¢«è½‰æˆ 1)
                    # header=None ä»£è¡¨æˆ‘å€‘ä¸é è¨­ç¬¬ä¸€åˆ—æ˜¯æ¨™é¡Œï¼Œç›´æ¥çœ‹åº§æ¨™
                    df_dict = pd.read_excel(uploaded_xlsx, sheet_name=None, header=None, dtype=str)
                    
                    st.session_state.source_mode = 'excel'
                    st.session_state.last_loaded_xlsx_name = current_file_name
                    
                    # æº–å‚™ä¸€å€‹å®¹å™¨ä¾†è£ã€Œå½è£æˆ AI è¼¸å‡ºã€çš„çµæœ
                    fake_ai_result = {
                        "header_info": {},
                        "summary_rows": [],
                        "dimension_data": [],
                        "issues": [],
                        "_token_usage": {"input": 0, "output": 0} # å‡è£æ²’èŠ±éŒ¢
                    }
                    
                    # ç”¨ä¾†é¡¯ç¤ºåœ–ç‰‡é è¦½çš„ list
                    st.session_state.photo_gallery = []

                    # --- é–‹å§‹è§£ææ¯ä¸€å€‹ Sheet ---
                    for sheet_name, df in df_dict.items():
                        # æ¸…æ´—æ•¸æ“šï¼šå¡«è£œç©ºå€¼ï¼Œç§»é™¤æ›è¡Œ
                        df = df.fillna("").astype(str)
                        df = df.replace(r'\n', '', regex=True).replace(r'\r', '', regex=True)
                        
                        # è½‰æˆ List of Lists æ¯”è¼ƒå¥½æ“ä½œåº§æ¨™
                        rows = df.values.tolist()
                        
                        # æš«å­˜è®Šæ•¸
                        current_item_title = None
                        current_std_spec = None
                        
                        # --- æƒææ¯ä¸€åˆ— ---
                        for r_idx, row in enumerate(rows):
                            row_str = "".join(row).replace(" ", "") # è©²åˆ—æ‰€æœ‰æ–‡å­—é»åœ¨ä¸€èµ·æ–¹ä¾¿æª¢æŸ¥
                            
                            # 1. æŠ“è¡¨é ­ (Header Info)
                            # é‚è¼¯ï¼šæª¢æŸ¥é€™ä¸€åˆ—æœ‰æ²’æœ‰é—œéµå­—ï¼Œå¦‚æœæœ‰ï¼ŒæŠ“å®ƒå³é‚Šé‚£ä¸€æ ¼
                            for c_idx, cell in enumerate(row):
                                cell_clean = str(cell).replace(" ", "").replace(":", "").replace("ï¼š", "")
                                if "å·¥ä»¤" in cell_clean and (c_idx + 1 < len(row)):
                                    # åªæœ‰ç•¶é‚„æ²’æŠ“åˆ°ï¼Œæˆ–æŠ“åˆ°çš„æ˜¯ Unknown æ™‚æ‰æ›´æ–°
                                    if not fake_ai_result["header_info"].get("job_no"):
                                        val = str(row[c_idx+1]).strip()
                                        if val: fake_ai_result["header_info"]["job_no"] = val
                                        
                                if "é å®š" in cell_clean and (c_idx + 1 < len(row)):
                                    fake_ai_result["header_info"]["scheduled_date"] = str(row[c_idx+1]).strip()
                                    
                                if "å¯¦éš›" in cell_clean or "å®Œæˆäº¤è²¨" in cell_clean:
                                    if c_idx + 1 < len(row):
                                        fake_ai_result["header_info"]["actual_date"] = str(row[c_idx+1]).strip()

                            # 2. æŠ“ç¸½è¡¨ (Summary)
                            # é‚è¼¯ï¼šé€šå¸¸æœƒæœ‰ã€Œé …ç›®åç¨±ã€ã€ã€Œç”³è«‹ã€ã€ã€Œå¯¦äº¤ã€åœ¨åŒä¸€åˆ—æˆ–é™„è¿‘
                            # é€™è£¡ç°¡åŒ–é‚è¼¯ï¼šå¦‚æœè©²åˆ—ç¬¬0æ ¼æœ‰æ±è¥¿ï¼Œä¸”å¾Œé¢æ ¼å­æœ‰æ•¸å­—ï¼Œä¸”ä¸æ˜¯ã€Œè¦ç¯„/æ¨™æº–ã€ç­‰å­—çœ¼
                            # (é€™éƒ¨åˆ†ä¾æ“šä½ çš„ Excel å¯¦éš›ç‹€æ³å¯èƒ½éœ€è¦å¾®èª¿åº§æ¨™)
                            if "ç”³è«‹" in row_str and "å¯¦äº¤" in row_str:
                                # é€™æ˜¯ç¸½è¡¨æ¨™é¡Œåˆ—ï¼Œè·³é
                                continue
                                
                            # å‡è¨­ç¸½è¡¨åœ¨ä¸Šæ–¹ï¼Œä¸”ç‰¹å¾µæ˜¯ï¼šç¬¬2æ¬„æ˜¯ç”³è«‹é‡ï¼Œç¬¬3æ¬„æ˜¯å¯¦äº¤é‡ (ä¾ç…§å¸¸è¦‹ Excel æ ¼å¼çŒœæ¸¬)
                            # ä½ å¯èƒ½éœ€è¦æ ¹æ“šå¯¦éš› Excel æ¬„ä½ index ä¿®æ”¹é€™è£¡çš„ [1], [2]
                            # é€™è£¡å¯«ä¸€å€‹ç°¡å–®çš„å•Ÿç™¼å¼æœå°‹ï¼š
                            if len(row) > 3 and r_idx < 15: # å‡è¨­ç¸½è¡¨åœ¨å‰15åˆ—
                                try:
                                    # å˜—è©¦æ‰¾çœ‹èµ·ä¾†åƒæ•¸å­—çš„æ¬„ä½
                                    col_title = row[0] # å‡è¨­ç¬¬ä¸€æ¬„æ˜¯æ¨™é¡Œ
                                    col_apply = row[1] # å‡è¨­ç¬¬äºŒæ¬„æ˜¯ç”³è«‹
                                    col_deliv = row[2] # å‡è¨­ç¬¬ä¸‰æ¬„æ˜¯å¯¦äº¤
                                    
                                    # ç°¡å–®åˆ¤æ–·ï¼šæ¨™é¡Œæœ‰å­—ï¼Œä¸”ç”³è«‹/å¯¦äº¤çœ‹èµ·ä¾†åƒæ•¸å­—
                                    if col_title and any(k in col_title for k in ["W", "R", "O", "Y", "è»¸", "è¼ª", "å¥—"]): 
                                        if re.match(r"^\d+\.?\d*$", str(col_apply)) and re.match(r"^\d+\.?\d*$", str(col_deliv)):
                                            fake_ai_result["summary_rows"].append({
                                                "page": sheet_name,
                                                "title": str(col_title).strip(),
                                                "apply_qty": float(col_apply),
                                                "delivery_qty": float(col_deliv)
                                            })
                                except: pass

                            # 3. æŠ“æ˜ç´° (Detail) - é€™æ˜¯é‡é»
                            # é‚è¼¯ï¼šå·¦é‚Šç¬¬ä¸€æ¬„(index 0) æ˜¯é …ç›®åç¨±ï¼Œä¸‹ä¸€åˆ—çš„ç¬¬ä¸€æ¬„æ˜¯è¦ç¯„
                            first_cell = str(row[0]).strip()
                            
                            # åˆ¤æ–·æ˜¯å¦ç‚ºã€Œé …ç›®åç¨±ã€åˆ—
                            # æ¢ä»¶ï¼šä¸æ˜¯ç©ºå€¼ï¼Œä¸æ˜¯é—œéµå­—ï¼Œä¸”é•·åº¦è¶³å¤ 
                            skip_keywords = ["è¦ç¯„", "è¦æ ¼", "æ¨™æº–", "å°ºå¯¸", "æª¢é©—", "é …æ¬¡", "å·¥ä»¤", "æ—¥æœŸ", "ç”³è«‹", "å¯¦äº¤", "å‚™è¨»"]
                            is_title_row = first_cell and not any(k in first_cell for k in skip_keywords)
                            
                            if is_title_row:
                                # æ‰¾åˆ°æ–°é …ç›®ï¼
                                current_item_title = first_cell
                                current_std_spec = "" # é‡ç½®è¦æ ¼ï¼Œç­‰å¾…ä¸‹ä¸€è¡Œè®€å–
                                
                                # é †ä¾¿æ‰¾ç›®æ¨™å€¼ (4SET)
                                target = 0
                                match_target = re.search(r"[ï¼ˆ(](\d+)[)ï¼‰]", current_item_title)
                                if match_target:
                                    target = int(match_target.group(1))
                                
                                # é å…ˆå»ºç«‹è³‡æ–™ç‰©ä»¶
                                item_data = {
                                    "page": sheet_name,
                                    "item_title": current_item_title,
                                    "std_spec": "", # ç¨å¾Œå¡«å…¥
                                    "item_pc_target": target,
                                    "batch_total_qty": 0,
                                    "category": None,
                                    "ds": ""
                                }
                                fake_ai_result["dimension_data"].append(item_data)
                                
                                # é€™ä¸€åˆ—å³é‚Šå¯èƒ½æœ‰æ•¸æ“š (ID: Value)
                                # å‡è¨­å¾ç¬¬ 1 æ¬„é–‹å§‹å¾€å³éƒ½æ˜¯æ•¸æ“šå€
                                ds_pairs = []
                                for i in range(1, len(row)-1, 2): # è·³è‘—è®€ï¼šID, Val, ID, Val...
                                    rid = str(row[i]).strip()
                                    val = str(row[i+1]).strip()
                                    if rid and val:
                                        ds_pairs.append(f"{rid}:{val}")
                                
                                if ds_pairs and fake_ai_result["dimension_data"]:
                                     fake_ai_result["dimension_data"][-1]["ds"] = "|".join(ds_pairs)

                            elif "è¦ç¯„" in first_cell or "è¦æ ¼" in first_cell or "æ¨™æº–" in first_cell:
                                # é€™æ˜¯ä¸Šä¸€é …ç›®çš„ã€Œè¦æ ¼åˆ—ã€
                                if fake_ai_result["dimension_data"]: # ç¢ºä¿æœ‰ä¸Šä¸€é …
                                    # æœ‰æ™‚å€™è¦æ ¼æœƒå¯«åœ¨ç¬¬ä¸€æ¬„ï¼Œæœ‰æ™‚å€™åœ¨ç¬¬äºŒæ¬„ï¼Œé€™è£¡æŠŠæ•´åˆ—æ–‡å­—æ¥èµ·ä¾†ç•¶è¦æ ¼
                                    spec_text = " ".join([str(x) for x in row if x]).replace("è¦ç¯„æ¨™æº–", "").strip()
                                    fake_ai_result["dimension_data"][-1]["std_spec"] = spec_text
                                    
                                    # è¦æ ¼åˆ—çš„å³é‚Šä¹Ÿå¯èƒ½æœ‰æ•¸æ“šï¼(ID: Value)
                                    # æ¥çºŒä¸Šä¸€é …çš„ ds
                                    current_ds = fake_ai_result["dimension_data"][-1]["ds"]
                                    extra_pairs = []
                                    for i in range(1, len(row)-1, 2):
                                        rid = str(row[i]).strip()
                                        val = str(row[i+1]).strip()
                                        if rid and val and rid not in ["è¦ç¯„æ¨™æº–", "è¦æ ¼"]:
                                            extra_pairs.append(f"{rid}:{val}")
                                    
                                    if extra_pairs:
                                        if current_ds:
                                            fake_ai_result["dimension_data"][-1]["ds"] += "|" + "|".join(extra_pairs)
                                        else:
                                            fake_ai_result["dimension_data"][-1]["ds"] = "|".join(extra_pairs)

                            else:
                                # æ—¢ä¸æ˜¯æ¨™é¡Œä¹Ÿä¸æ˜¯è¦æ ¼ï¼Œå¯èƒ½æ˜¯ç´”æ•¸æ“šåˆ— (ä¾‹å¦‚ IDå¤ªå¤šæ›è¡Œäº†)
                                # å¦‚æœç›®å‰æœ‰æ­£åœ¨è™•ç†çš„é …ç›®ï¼Œå˜—è©¦è®€å–å³é‚Šçš„æ ¼å­
                                if current_item_title and fake_ai_result["dimension_data"]:
                                    more_pairs = []
                                    # å¾ç¬¬ 1 æ¬„é–‹å§‹æƒ
                                    for i in range(1, len(row)-1, 2):
                                        rid = str(row[i]).strip()
                                        val = str(row[i+1]).strip()
                                        # ç°¡å–®éæ¿¾é›œè¨Š
                                        if rid and val and len(rid) < 10 and len(val) < 10:
                                            more_pairs.append(f"{rid}:{val}")
                                    
                                    if more_pairs:
                                        current_ds = fake_ai_result["dimension_data"][-1]["ds"]
                                        if current_ds:
                                            fake_ai_result["dimension_data"][-1]["ds"] += "|" + "|".join(more_pairs)
                                        else:
                                            fake_ai_result["dimension_data"][-1]["ds"] = "|".join(more_pairs)

                        # å»ºç«‹é è¦½æ–‡å­— (Optional)
                        md_table = df.to_markdown(index=False)
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': md_table,
                            'header_text': f"ä¾†æºåˆ†é : {sheet_name}",
                            'full_text': f"Excel ç›´è®€æ¨¡å¼ - {sheet_name}",
                            'raw_json': None,
                            'real_page': sheet_name
                        })

                    # --- [é—œéµ] å°‡ç›´è®€çµæœå­˜å…¥ Cacheï¼Œè·³é AI ---
                    # æˆ‘å€‘ç›´æ¥æ§‹é€ ä¸€å€‹å®Œæ•´çš„ cache ç‰©ä»¶ï¼Œé¨™éå¾Œé¢çš„ç¨‹å¼
                    st.session_state.analysis_result_cache = {
                        "job_no": fake_ai_result["header_info"].get("job_no", "Unknown"),
                        "header_info": fake_ai_result["header_info"],
                        "summary_rows": fake_ai_result["summary_rows"],
                        "dimension_data": fake_ai_result["dimension_data"],
                        "issues": [], # Excel ç›´è®€é€šå¸¸æ²’æœ‰ AI è§£æéŒ¯èª¤
                        "_token_usage": {"input": 0, "output": 0},
                        
                        # è£œä¸Šè¨ˆæ™‚è³‡è¨Š (é€™æ˜¯ Python é‹ç®—æ‰€éœ€)
                        "total_duration": 0.5,
                        "ocr_duration": 0,
                        "ai_duration": 0,
                        "py_duration": 0,
                        "cost_twd": 0,
                        "total_in": 0, 
                        "total_out": 0,
                        
                        "ai_extracted_data": fake_ai_result["dimension_data"],
                        "full_text_for_search": "Excel Direct Read",
                        "combined_input": "Excel Direct Read"
                    }
                    
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ Excel ä¸¦å®Œæˆè§£æ: {current_file_name}", icon="âš¡")
                    
                    # ğŸ”¥ [é‡è¦] é€™è£¡ç›´æ¥è§¸ç™¼ rerunï¼Œè®“ UI è®€å–å‰›å‰›å­˜é€² cache çš„è³‡æ–™
                    # ä½†ç‚ºäº†è®“ Python é‚è¼¯ (check) è·‘ä¸€æ¬¡ï¼Œæˆ‘å€‘è¨­å®š auto_start = True
                    # å¯æ˜¯å› ç‚ºæˆ‘å€‘å·²ç¶“æŠŠçµæœåšå¥½å¡é€² cache äº†ï¼Œå…¶å¯¦åªè¦æŒ‰ä¸‹ã€Œé–‹å§‹åˆ†æã€æ™‚
                    # æˆ‘å€‘å¯ä»¥å¯«ä¸€å€‹åˆ¤æ–·ï¼šå¦‚æœæ˜¯ Excel æ¨¡å¼ï¼Œç›´æ¥è·³é AI å‘¼å«ï¼Œåªè·‘ Python check
                    st.session_state.auto_start_analysis = True 
                    st.rerun()
                    
                else:
                    st.success(f"ğŸ“Š ç›®å‰è¼‰å…¥ Excelï¼š**{uploaded_xlsx.name}**")
            except Exception as e:
                st.error(f"Excel è§£æå¤±æ•—: {e}")

if st.session_state.photo_gallery:
    st.caption(f"å·²ç´¯ç© {len(st.session_state.photo_gallery)} é æ–‡ä»¶")
    col_btn1, col_btn2 = st.columns([1, 1], gap="small")
    with col_btn1: start_btn = st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary", use_container_width=True)
    with col_btn2: 
        clear_btn = st.button("ğŸ—‘ï¸ç…§ç‰‡æ¸…é™¤", help="æ¸…é™¤", use_container_width=True)

    if clear_btn:
        st.session_state.photo_gallery = []
        st.session_state.analysis_result_cache = None
        if 'last_loaded_json_name' in st.session_state:
            del st.session_state.last_loaded_json_name 
        st.rerun()

    is_auto_start = st.session_state.auto_start_analysis
    if is_auto_start:
        st.session_state.auto_start_analysis = False

    if 'analysis_result_cache' not in st.session_state:
        st.session_state.analysis_result_cache = None

    trigger_analysis = start_btn or is_auto_start

    if trigger_analysis:
        # --- [ä¿®æ”¹ 1] æ™ºæ…§æ¸…é™¤ Cache ---
        # å¦‚æœæ˜¯ Excel ç›´è®€æ¨¡å¼ä¸”å·²ç¶“æœ‰çµæœ (å‰›ä¸Šå‚³å®Œ)ï¼Œå°±ä¸è¦æ¸…é™¤ Cacheï¼Œå¦å‰‡æ•¸æ“šæœƒä¸è¦‹ï¼
        # å…¶ä»–æ¨¡å¼ (ç…§ç‰‡/JSON) å‰‡å¼·åˆ¶æ¸…é™¤ï¼Œç¢ºä¿æ˜¯æ–°çš„åˆ†æ
        is_excel_direct_mode = (st.session_state.get('source_mode') == 'excel' and st.session_state.analysis_result_cache)
        
        if not is_excel_direct_mode:
            st.session_state.analysis_result_cache = None 
            
        st.session_state.auto_start_analysis = False
        total_start = time.time()
        
        with st.status("ç¸½ç¨½æ ¸å®˜æ­£åœ¨é€²è¡Œå…¨æ–¹ä½åˆ†æ...", expanded=True) as status_box:
            progress_bar = st.progress(0)
            
            # åˆå§‹åŒ–è®Šæ•¸ (ç¢ºä¿å¾Œé¢ Python é‚è¼¯æœ‰æ±è¥¿å¯è®€)
            res_main = {}
            ocr_duration = 0
            ai_duration = 0
            combined_input = ""

            # ==========================================
            # ğŸ”€ åˆ†æµåˆ¤æ–·ï¼šExcel ç›´è®€ vs AI åˆ†æ
            # ==========================================
            if is_excel_direct_mode:
                status_box.write("âš¡ åµæ¸¬åˆ° Excel ç›´è®€æ•¸æ“šï¼Œè·³é AI åˆ†æï¼Œç›´æ¥åŸ·è¡Œé‚è¼¯ç¨½æ ¸...")
                time.sleep(0.5) # çµ¦å€‹è¦–è¦ºç·©è¡
                
                # ç›´æ¥å¾ Cache æ‹¿è³‡æ–™
                res_main = st.session_state.analysis_result_cache
                combined_input = res_main.get("combined_input", "Excel Direct Read")
                
                # æ¨¡æ“¬é€²åº¦æ¢è·‘å®Œ
                progress_bar.progress(0.4)
                
            else:
                # ==========================================
                # æ–¹æ¡ˆ A: æ¨™æº– AI æµç¨‹ (OCR + Gemini)
                # ==========================================
                
                # 1. OCR
                status_box.write("ğŸ‘€ æ­£åœ¨é€²è¡Œ OCR æ–‡å­—è­˜åˆ¥...")
                ocr_start = time.time()
                
                def process_task(index, item):
                    if item.get('full_text'): return index, item.get('header_text',''), item['full_text'], None
                    try:
                        item['file'].seek(0)
                        _, h, f, _, _ = extract_layout_with_azure(item['file'], DOC_ENDPOINT, DOC_KEY)
                        return index, h, f, None
                    except Exception as e: return index, None, None, str(e)

                with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                    futures = [executor.submit(process_task, i, item) for i, item in enumerate(st.session_state.photo_gallery)]
                    for future in concurrent.futures.as_completed(futures):
                        idx, h_txt, f_txt, err = future.result()
                        if not err:
                            st.session_state.photo_gallery[idx].update({'header_text': h_txt, 'full_text': f_txt, 'file': None})
                        progress_bar.progress(0.4 * ((idx + 1) / len(st.session_state.photo_gallery)))

                ocr_duration = time.time() - ocr_start
                
                # 2. çµ„åˆæ–‡å­—
                combined_input = ""
                for i, p in enumerate(st.session_state.photo_gallery):
                    combined_input += f"\n=== Page {i+1} ===\n{p.get('full_text','')}\n"

                # ==========================================
                # ğŸš€ 3. AI ä¸¦è¡Œåˆ†æ (Turbo Mode)
                # ==========================================
                status_box.write("ğŸ¤– AI æ­£åœ¨åˆ†æ‰¹ä¸¦è¡Œè™•ç† (Turbo Mode)...")
                ai_start_time = time.time()
                
                # 1. æº–å‚™æ‰¹æ¬¡
                all_pages = st.session_state.photo_gallery
                batches = list(split_into_batches(all_pages, max_size=3)) 
                
                ai_futures = []
                results_bucket = [None] * len(batches)

                # å®šç¾©ä¸€å€‹å­ä»»å‹™å‡½æ•¸
                def process_batch(batch_idx, batch_pages):
                    batch_text = ""
                    for p in batch_pages:
                        real_idx = all_pages.index(p) + 1 
                        batch_text += f"\n=== Page {real_idx} ===\n{p.get('full_text','')}\n"
                    
                    full_text_all = "".join([p.get('full_text','') for p in all_pages])
                    return agent_unified_check(batch_text, full_text_all, GEMINI_KEY, main_model_name)

                # 2. åŒæ™‚ç™¼å°„ç«ç®­
                with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                    for idx, batch in enumerate(batches):
                        future = executor.submit(process_batch, idx, batch)
                        ai_futures.append((idx, future))
                    
                    for idx, future in ai_futures:
                        try:
                            res = future.result()
                            results_bucket[idx] = res
                        except Exception as e:
                            results_bucket[idx] = {"header_info": {}, "summary_rows": [], "dimension_data": [], "issues": []}
                            st.error(f"Batch {idx+1} åˆ†æå¤±æ•—: {e}")

                # 3. æ‹¼æ¹Šçµæœ
                res_main = merge_ai_results(results_bucket)
                
                # æ›´æ–°å…¨å·æ–‡å­—ä¾› Cache ä½¿ç”¨
                combined_input = ""
                for i, p in enumerate(all_pages):
                    combined_input += f"\n=== Page {i+1} ===\n{p.get('full_text','')}\n"
                
                ai_duration = time.time() - ai_start_time

            # ========================================================
            # ğŸ æµç¨‹åŒ¯åˆï¼šä»¥ä¸‹é‚è¼¯ç„¡è«–æ˜¯ Excel é‚„æ˜¯ AI éƒ½æœƒåŸ·è¡Œ
            # ========================================================
            
            # ğŸ”¥ æ’å…¥é»ï¼šè³‡æ–™ä¿®å¾©æµæ°´ç·š (çµæ§‹ä¿®å¾© -> èªæ„ä¿®å¾©)
            raw_dim_data = res_main.get("dimension_data", [])
            
            # æ­¥é©Ÿ 1: åŸ·è¡Œç¾…è³“æ¼¢ (ä¿®å¾©çµæ§‹)
            balanced_dim_data = rebalance_orphan_data(raw_dim_data)
            
            # æ­¥é©Ÿ 2: åŸ·è¡Œå¼·åˆ¶æ›´å (ä¿®å¾©èªæ„/ç­†èª¤)
            final_dim_data = apply_forced_renaming(balanced_dim_data)
            
            # æ­¥é©Ÿ 3: å›å­˜æœ€çµ‚çµæœ
            res_main["dimension_data"] = final_dim_data
            
            # ========================================================
            # ğŸ”¥ æ’å…¥é»ï¼šè³‡æ–™ä¿®å¾©æµæ°´ç·š (çµæ§‹ä¿®å¾© -> èªæ„ä¿®å¾©)
            # ========================================================
            raw_dim_data = res_main.get("dimension_data", [])
            
            # æ­¥é©Ÿ 1: åŸ·è¡Œç¾…è³“æ¼¢ (ä¿®å¾©çµæ§‹)
            # å…ˆè§£æ±ºè¦–è¦ºæ–·è¡Œèª¤åˆ¤ (ä¾‹å¦‚ 7å€‹è®Š12å€‹çš„å•é¡Œ)
            balanced_dim_data = rebalance_orphan_data(raw_dim_data)
            
            # æ­¥é©Ÿ 2: åŸ·è¡Œå¼·åˆ¶æ›´å (ä¿®å¾©èªæ„/ç­†èª¤)
            # è®€å– Excel Force_Renameï¼ŒæŠŠ "è»¸é ¸å†ç”Ÿ" å¼·åˆ¶æ”¹åç‚º "è»¸é ¸éŠ²è£œ"
            # å‚³å…¥çš„æ˜¯å·²ç¶“çµæ§‹æ­£ç¢ºçš„ balanced_dim_data
            final_dim_data = apply_forced_renaming(balanced_dim_data)
            
            # æ­¥é©Ÿ 3: å›å­˜æœ€çµ‚çµæœ (ç¢ºä¿å¾ŒçºŒæ‰€æœ‰æµç¨‹éƒ½ç”¨æ–°åå­—)
            res_main["dimension_data"] = final_dim_data
            # ========================================================

            # 4. Python é‚è¼¯æª¢æŸ¥ (åŠ å…¥è¨ˆæ™‚)
            status_box.write("ğŸ Python æ­£åœ¨é€²è¡Œé‚è¼¯æ¯”å°...")
            
            py_start_time = time.time() # â±ï¸ [è¨ˆæ™‚é–‹å§‹] Python
            
            # é€™è£¡ç›´æ¥å–ç”¨å‰›å‰›ä¿®å¾©ä¸¦æ”¹åå¾Œçš„ final_dim_data (å¾ res_main æ‹¿)
            dim_data = res_main.get("dimension_data", [])
            
            # é‡æ–°è·‘åˆ†é¡ (é‡è¦ï¼å› ç‚ºåå­—å‰›è¢«æˆ‘å€‘æ”¹æˆéŠ²è£œï¼Œé€™è£¡åˆ†é¡å°±æœƒè‡ªå‹•è®ŠæˆéŠ²è£œ)
            for item in dim_data:
                new_cat = assign_category_by_python(item.get("item_title", ""))
                item["category"] = new_cat
                if "sl" not in item: item["sl"] = {}
                item["sl"]["lt"] = new_cat
            
            # é–‹å§‹å„é …ç¨½æ ¸ (å‚³å…¥ä¿®å¾©å¾Œçš„è³‡æ–™)
            python_numeric_issues = python_numerical_audit(dim_data)
            python_accounting_issues = python_accounting_audit(dim_data, res_main)
            python_process_issues = python_process_audit(dim_data)
            python_header_issues = python_header_audit_batch(st.session_state.photo_gallery, res_main)

            # ğŸ”¥ [é—œéµè£œæ•‘] é€™ä¸€å¡Šå¿…é ˆç•™è‘—ï¼ä¸èƒ½å…¨åˆªï¼
            ai_filtered_issues = []
            ai_raw_issues = res_main.get("issues", [])
            if isinstance(ai_raw_issues, list):
                for i in ai_raw_issues:
                    if isinstance(i, dict):
                        i['source'] = 'ğŸ¤– ç¸½ç¨½æ ¸ AI'
                        # éæ¿¾æ‰ä¸€äº›æ²’ç”¨çš„ AI é›œè¨Š
                        if not any(k in i.get("issue_type", "") for k in ["æµç¨‹", "è¦æ ¼æå–å¤±æ•—", "æœªåŒ¹é…"]):
                            ai_filtered_issues.append(i)

            # ğŸ”¥ é€™è£¡åŸ·è¡Œåˆä½µ (ç¾åœ¨ ai_filtered_issues å·²ç¶“å¾©æ´»äº†ï¼Œä¸æœƒå†å ±éŒ¯)
            all_issues = ai_filtered_issues + python_numeric_issues + python_accounting_issues + python_process_issues + python_header_issues
            
            py_duration = time.time() - py_start_time # â±ï¸ [è¨ˆæ™‚çµæŸ] Python

            # 5. å­˜æª” (Cache)
            usage = res_main.get("_token_usage", {"input": 0, "output": 0})
            
            # ä¿®æ­£å·¥ä»¤è®€å–é‚è¼¯
            final_job_no = res_main.get("header_info", {}).get("job_no")
            if not final_job_no or final_job_no == "Unknown":
                 final_job_no = res_main.get("job_no", "Unknown")
            
            st.session_state.analysis_result_cache = {
                "job_no": final_job_no,
                "header_info": res_main.get("header_info", {}),
                "all_issues": all_issues,
                "total_duration": time.time() - total_start,
                "ocr_duration": ocr_duration,
                "ai_duration": ai_duration,     # AI è€—æ™‚
                "py_duration": py_duration,     # Python è€—æ™‚
                
                "cost_twd": (usage.get("input", 0)*0.3 + usage.get("output", 0)*2.5) / 1000000 * 32.5,
                "total_in": usage.get("input", 0),
                "total_out": usage.get("output", 0),
                
                "ai_extracted_data": dim_data,
                "freight_target": res_main.get("freight_target", 0),
                "summary_rows": res_main.get("summary_rows", []),
                "full_text_for_search": combined_input,
                "combined_input": combined_input
            }
            
            progress_bar.progress(1.0)
            status_box.update(label="âœ… åˆ†æå®Œæˆï¼", state="complete", expanded=False)
            st.rerun()

       # --- ğŸ’¡ é¡¯ç¤ºçµæœå€å¡Š ---
    if st.session_state.analysis_result_cache:
        cache = st.session_state.analysis_result_cache
        all_issues = cache.get('all_issues', [])

        # --- ğŸ“‹ è¡¨é ­è³‡è¨Šåµæ¸¬ (æ‰‹æ©Ÿç‰ˆå¼·è£½æ©«æ’å„ªåŒ–) ---
        st.divider()
        st.subheader("ğŸ“‹ è¡¨é ­è³‡è¨Šåµæ¸¬")
        
        h_info = cache.get("header_info", {}) 
        current_job = h_info.get("job_no", "æœªåµæ¸¬")
        sch_date = h_info.get("scheduled_date", "æœªåµæ¸¬")
        act_date = h_info.get("actual_date", "æœªåµæ¸¬")

        # 1. å…ˆè™•ç†ç´…è‰²è­¦ç¤ºçš„ HTML æ¨£å¼å­—ä¸²
        act_date_html = f"<b>{act_date}</b>"
        try:
            if act_date != "æœªåµæ¸¬" and sch_date != "æœªåµæ¸¬" and act_date > sch_date:
                # å¦‚æœé€¾æœŸï¼Œè®Šç´…è‰² (#ff4b4b æ˜¯ Streamlit çš„æ¨™æº–ç´…)
                act_date_html = f"<b style='color: #ff4b4b;'>{act_date} (é€¾æœŸ)</b>"
        except: pass

        # 2. ä½¿ç”¨ HTML Flexbox å¼·åˆ¶æ©«å‘æ’åˆ—
        st.markdown(f"""
        <div style="display: flex; flex-direction: row; justify-content: space-between; width: 100%;">
            <div style="flex: 1; padding-right: 5px;">
                <div style="font-size: 12px; color: gray; margin-bottom: 2px;">å·¥ä»¤å–®è™Ÿ</div>
                <div style="font-size: 16px; font-weight: bold;">{current_job}</div>
            </div>
            <div style="flex: 1; padding-right: 5px;">
                <div style="font-size: 12px; color: gray; margin-bottom: 2px;">é å®šäº¤è²¨æ—¥</div>
                <div style="font-size: 16px; font-weight: bold;">{sch_date}</div>
            </div>
            <div style="flex: 1;">
                <div style="font-size: 12px; color: gray; margin-bottom: 2px;">å¯¦éš›äº¤è²¨æ—¥</div>
                <div style="font-size: 16px;">{act_date_html}</div>
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        st.divider()

        # 3. é ‚éƒ¨ç‹€æ…‹æ¢ (ä¿®æ”¹ç‰ˆï¼šè©³ç´°æ™‚é–“æ‹†è§£)
        # æ ¼å¼ï¼šç¸½è€—æ™‚ (OCR | AI | Python)
        total_t = cache.get('total_duration', 0)
        ocr_t = cache.get('ocr_duration', 0)
        ai_t = cache.get('ai_duration', 0)
        py_t = cache.get('py_duration', 0)
        
        st.success(
            f"ç¸½è€—æ™‚: {total_t:.1f}s  "
            f"( OCR: {ocr_t:.1f}s | AI: {ai_t:.1f}s | Py: {py_t:.2f}s )"
        )
        
        st.info(f"ğŸ’° æœ¬æ¬¡æˆæœ¬: NT$ {cache['cost_twd']:.2f} (In: {cache['total_in']:,} / Out: {cache['total_out']:,})")
        
        # 4. è¦å‰‡å±•ç¤º (v58: å®Œæ•´æ¬„ä½å…­å®®æ ¼ç‰ˆ)
        with st.expander("ğŸ—ï¸ æª¢è¦– Excel é‚è¼¯èˆ‡è¦å‰‡åƒæ•¸", expanded=False):
            
            # 1. ä¿®æ­£è³‡æ–™æºï¼šæ”¹è®€ analysis_result_cache
            target_list = []
            if st.session_state.analysis_result_cache:
                target_list = st.session_state.analysis_result_cache.get('all_issues', [])
            
            # 2. æ‰¾å‡ºéš±è—åŒ…è£¹ (HIDDEN_DATA)
            hidden_payload = {}
            for item in target_list:
                if item.get('issue_type') == 'HIDDEN_DATA':
                    hidden_payload = item
                    break
            
            # 3. è§£æè³‡æ–™
            rule_hits = hidden_payload.get('rule_hits', {})
            current_fuzz = globals().get('GLOBAL_FUZZ_THRESHOLD', hidden_payload.get('fuzz_threshold', 90))

            st.caption(f"â„¹ï¸ å…¨åŸŸçµ±ä¸€ç‰¹è¦é–€æª»: **{current_fuzz} åˆ†**")
            
            try:
                # å˜—è©¦è®€å– Excel æª”æ¡ˆ
                df_rules = pd.read_excel("rules.xlsx")
                df_rules.columns = [c.strip() for c in df_rules.columns]
                
                # å»ºç«‹å¿«é€ŸæŸ¥è©¢è¡¨
                rule_info_map = {}
                rules_map_for_xray = {} 
                
                for _, row in df_rules.iterrows():
                    r_name = str(row.get('Item_Name', '')).strip()
                    clean_k = r_name.replace(" ", "").replace("\n", "").replace("\r", "").replace('"', '').replace("'", "").strip()
                    rule_info_map[clean_k] = row
                    rules_map_for_xray[clean_k] = row

                # 4. é¡¯ç¤ºçµæœ (å¦‚æœæœ‰å‘½ä¸­)
                if rule_hits:
                    st.success(f"ğŸ¯ ç³»çµ±åµæ¸¬åˆ° {len(rule_hits)} ç¨®ç‰¹è¦é …ç›®ï¼")
                    
                    for rule_key, hits in rule_hits.items():
                        info = rule_info_map.get(rule_key, {})
                        
                        st.markdown(f"#### âœ… {rule_key}")
                        
                        # ğŸ”¥ğŸ”¥ğŸ”¥ [ç‰ˆé¢ä¿®æ”¹] æ”¹ç‚º 2 æ¬„æ’åˆ—ï¼Œé¡¯ç¤º 6 å€‹æ¬„ä½ ğŸ”¥ğŸ”¥ğŸ”¥
                        c_left, c_right = st.columns(2)
                        
                        with c_left:
                            st.markdown(f"**Local:** `{info.get('Unit_Rule_Local', '-')}`")
                            st.markdown(f"**Freight:** `{info.get('Unit_Rule_Freight', '-')}`")
                            st.markdown(f"**Agg:** `{info.get('Unit_Rule_Agg', '-')}`")
                            
                        with c_right:
                            # å˜—è©¦è®€å–æ›´å¤šæ¬„ä½ï¼Œè‹¥ Excel æ²’é€™æ¬„ä½æœƒé¡¯ç¤º '-'
                            st.markdown(f"**Category:** `{info.get('Category', '-')}`")
                            st.markdown(f"**Process:** `{info.get('Process_Rule', '-')}`")
                            # ğŸ”¥ æ”¹æˆé¡¯ç¤º Force_Rename
                            st.markdown(f"**Rename:** `{info.get('Force_Rename', '-')}`") 
                        # -----------------------------------------------------
                        
                        # é¡¯ç¤ºæ˜ç´°è¡¨æ ¼
                        hit_df = pd.DataFrame(hits)
                        cols_to_show = ["æ˜ç´°åç¨±", "åˆ†æ•¸", "åŒ¹é…é¡å‹", "é ç¢¼"]
                        final_cols = [c for c in cols_to_show if c in hit_df.columns]
                        
                        if "åˆ†æ•¸" in final_cols:
                            st.dataframe(hit_df[final_cols].style.format({"åˆ†æ•¸": "{:.0f}"}), use_container_width=True, hide_index=True)
                        else:
                            st.dataframe(hit_df, use_container_width=True, hide_index=True)
                else:
                    if target_list:
                        st.info(f"æœ¬æ¬¡å·¥ä»¤æœªè§¸ç™¼ä»»ä½•ç‰¹è¦é …ç›® (é–€æª»: {current_fuzz})ã€‚")
                    else:
                        st.warning("âš ï¸ å°šæœªåŸ·è¡Œåˆ†ææˆ–ç„¡åˆ†æçµæœã€‚")

                # åº•éƒ¨ï¼šå®Œæ•´çš„è¦å‰‡ç¸½è¡¨
                st.markdown("---")
                with st.expander("ğŸ“‹ æŸ¥çœ‹å®Œæ•´è¦å‰‡ç¸½è¡¨ (All Rules)", expanded=False):
                    st.dataframe(df_rules, use_container_width=True, hide_index=True)

                # ğŸ”¥ Xå…‰æ©Ÿ (ä¿ç•™)
                st.markdown("---")
                st.subheader("ğŸ•µï¸â€â™‚ï¸ Xå…‰æª¢æ¸¬ï¼šç‚ºä»€éº¼æ²’æŠ“åˆ°ï¼Ÿ")
                st.caption(f"é€™è£¡åˆ—å‡ºå‰ 10 ç­†é …ç›®çš„æœ€é«˜åˆ†è¦å‰‡ï¼Œå¹«æ‚¨æ±ºå®š GLOBAL_FUZZ_THRESHOLD è©²è¨­å¤šå°‘ (ç›®å‰: {current_fuzz})")
                
                sample_items = []
                acc_input = st.session_state.get('analysis_result_cache', {}).get('ai_extracted_data', [])
                if acc_input:
                    sample_items = [item.get('item_title', '') for item in acc_input[:10]]
                
                if sample_items:
                    debug_data = []
                    for item_title in sample_items:
                        clean_title = item_title.replace(" ", "").replace("\n", "").strip()
                        best_score = 0
                        best_rule = "ç„¡"
                        
                        # è¨˜å¾—é€™è£¡è¦è·Ÿæ‚¨æœ€å¾Œæ±ºå®šä½¿ç”¨çš„ fuzz æ–¹å¼åŒæ­¥ (ç›®å‰å»ºè­° token_sort_ratio)
                        for k in rules_map_for_xray.keys():
                            sc = fuzz.token_sort_ratio(k, clean_title)
                            if sc > best_score:
                                best_score = sc
                                best_rule = k
                        
                        status = "ğŸ”´ è½æ¦œ"
                        if best_score > current_fuzz: status = "ğŸŸ¢ éŒ„å–"
                        
                        debug_data.append({
                            "å·¥ä»¤é …ç›®": clean_title,
                            "æœ€åƒçš„è¦å‰‡": best_rule,
                            "è¨ˆç®—åˆ†æ•¸": best_score,
                            "ç‹€æ…‹": status
                        })
                    st.dataframe(pd.DataFrame(debug_data))

            except Exception as e:
                st.error(f"UI é¡¯ç¤ºéŒ¯èª¤: {e}")
                
        # 5. åŸå§‹æ•¸æ“šæª¢è¦–
        with st.expander("ğŸ“Š æª¢è¦– AI æŠ„éŒ„åŸå§‹æ•¸æ“š", expanded=False):
            st.markdown("**1. æ ¸å¿ƒæŒ‡æ¨™æ‘˜è¦**")
            sum_rows_len = len(cache.get("summary_rows", []))
            summary_df = pd.DataFrame([{
                "å·¥ä»¤å–®è™Ÿ": cache.get("job_no", "N/A"),
                "ç¸½è¡¨è¡Œæ•¸": sum_rows_len,
                "ç¸½è¡¨ç‹€æ…‹": "æ­£å¸¸" if sum_rows_len > 0 else "ç©ºå€¼"
            }])
            st.dataframe(summary_df, hide_index=True, use_container_width=True)
            st.divider()
 
            st.markdown("**2. å·¦ä¸Šè§’çµ±è¨ˆè¡¨ (Summary Rows)**")
            sum_rows = cache.get("summary_rows", [])
            
            if sum_rows:
                df_sum = pd.DataFrame(sum_rows)
                
                # 1. ç¢ºä¿é ç¢¼æ¬„ä½å­˜åœ¨
                if "page" not in df_sum.columns: df_sum["page"] = "?"
                
                # 2. æ¬„ä½æ›´å (å…¼å®¹èˆŠç‰ˆ target èˆ‡æ–°ç‰ˆ delivery_qty)
                rename_map = {
                    "page": "é ç¢¼", 
                    "title": "é …ç›®åç¨±", 
                    "apply_qty": "ç”³è«‹æ•¸é‡",    # âœ… æ–°å¢ï¼šç”³è«‹æ•¸é‡
                    "delivery_qty": "å¯¦äº¤æ•¸é‡", # âœ… æ–°å¢ï¼šå¯¦äº¤æ•¸é‡
                    "target": "å¯¦äº¤æ•¸é‡"        # èˆŠç‰ˆå…¼å®¹ (è‹¥ç„¡ delivery_qty å‰‡ç”¨ target)
                }
                df_sum.rename(columns=rename_map, inplace=True)
                
                # 3. æŒ‡å®šé¡¯ç¤ºé †åº (ç¢ºä¿æ¬„ä½ä¸æœƒæ¶ˆå¤±)
                # å…ˆåˆ—å‡ºæˆ‘å€‘æƒ³è¦çš„é †åº
                desired_cols = ["é ç¢¼", "é …ç›®åç¨±", "ç”³è«‹æ•¸é‡", "å¯¦äº¤æ•¸é‡"]
                # åªä¿ç•™ DataFrame ä¸­çœŸçš„å­˜åœ¨çš„æ¬„ä½
                final_cols = [c for c in desired_cols if c in df_sum.columns]
                
                st.dataframe(df_sum[final_cols], hide_index=True, use_container_width=True)
            else:
                st.caption("ç„¡æ•¸æ“š")

            st.divider()
            st.markdown("**3. å…¨å·è©³ç´°æŠ„éŒ„æ•¸æ“š (JSON)**")
            st.json(cache.get("ai_extracted_data", []), expanded=True)

        # ========================================================
        # âš¡ï¸ [æœ€çµ‚çµ±è¨ˆèˆ‡é¡¯ç¤ºå€å¡Š]ï¼šå¾¹åº•æ’é™¤éš±è—è³‡æ–™å°æ•¸é‡çš„å½±éŸ¿
        # ========================================================
        
        # 1. åŸ·è¡Œåˆä½µ (å°‡æ‰€æœ‰å¼•æ“çš„çµæœåŒ¯æ•´)
        consolidated_list = consolidate_issues(all_issues)

        # 2. ğŸ”¥ [æ ¸å¿ƒä¿®æ­£] å»ºç«‹ã€Œå¯è¦‹ç•°å¸¸æ¸…å–®ã€ï¼šæ’é™¤ HIDDEN_DATA
        # é€™æ¨£ä¹‹å¾Œçš„æ•¸é‡çµ±è¨ˆ (len) æ‰æœƒæ˜¯æ­£ç¢ºçš„
        visible_issues = [i for i in consolidated_list if i.get('issue_type') != 'HIDDEN_DATA']

        # 3. éæ¿¾å‡ºã€ŒçœŸæ­£çš„éŒ¯èª¤ã€(æ’é™¤åƒ…æ˜¯æç¤ºæ€§çš„ "æœªåŒ¹é…")
        real_errors = [i for i in visible_issues if "æœªåŒ¹é…" not in i.get('issue_type', '')]

        # 4. é¡¯ç¤ºçµè«– (æ”¹ç”¨ visible_issues èˆ‡ real_errors åˆ¤æ–·)
        if not visible_issues:
            # å¦‚æœæ‰£é™¤éš±è—è³‡æ–™å¾Œæ²’æ±è¥¿ï¼Œå°±æ˜¯çœŸçš„å…¨æ•¸åˆæ ¼
            st.balloons()
            st.success("âœ… å…¨æ•¸åˆæ ¼ï¼")
        elif not real_errors:
            # æœ‰é¡¯ç¤ºé …ç›®ï¼Œä½†éƒ½ä¸æ˜¯åš´é‡ç´…å­—ç•°å¸¸
            st.success(f"âœ… æ•¸å€¼åˆæ ¼ï¼ (ä½†æœ‰ {len(visible_issues)} é¡é …ç›®æœªåŒ¹é…è¦å‰‡)")
        else:
            # çœŸçš„æœ‰éœ€è¦ä¿®æ­£çš„ç´…å­—ç•°å¸¸
            st.error(f"ç™¼ç¾ {len(real_errors)} é¡ç•°å¸¸")

        # ========================================================
        # âœ… [æ–°å¢åŠŸèƒ½]ï¼šPython åˆ¤å®šåˆæ ¼/ç•°å¸¸ç¸½è¦½æ¸…å–®
        # ========================================================
        with st.expander("ğŸ§ æª¢è¦– Python å…¨é …ç›®åˆ¤å®š (åˆæ ¼/ç•°å¸¸æ¸…å–®)", expanded=False):
            
            # 1. æº–å‚™æ¯”å°ç”¨çš„é»‘åå–® (ç”¨ä¾†åˆ¤æ–·èª°æ˜¯ç´…ç‡ˆ)
            # æ ¼å¼ï¼š(é ç¢¼å­—ä¸², é …ç›®åç¨±)
            failed_set = set()
            for issue in visible_issues: # ä½¿ç”¨å·²ç¶“æ¿¾æ‰ HIDDEN_DATA çš„æ¸…å–®
                p_str = str(issue.get('page', '?')).strip()
                i_str = str(issue.get('item', '')).strip()
                # é‡å°ç¸½è¡¨ç•°å¸¸ï¼Œissue çš„ page é€šå¸¸æ˜¯ "ç¸½è¡¨" æˆ–ä¾†æºé ç¢¼
                failed_set.add((p_str, issue.get('item', '')))

            # å»ºç«‹åˆ†é 
            tab_sum, tab_det = st.tabs(["ğŸ“Š ç¸½è¡¨é …ç›® (Summary)", "ğŸ“ æ˜ç´°é …ç›® (Detail)"])

            # --- Tab 1: ç¸½è¡¨æª¢æŸ¥ (v3: å¼•æ“ç›´è®€ç‰ˆ) ---
            with tab_sum:
                raw_sum = cache.get("summary_rows", [])
                
                if raw_sum:
                    sum_data = []
                    
                    for row in raw_sum:
                        # ç›´æ¥è®€å–å¼•æ“å›å¯«çš„è³‡æ–™
                        mode = row.get('_audit_mode', 'æœªé‹ç®—')
                        details = row.get('_audit_details', [])
                        status = row.get('_audit_status', 'âšª æœªçŸ¥')
                        note = row.get('_audit_note', '')
                        
                        # 1. è™•ç†ã€Œåˆ—è¡¨é …ç›®ã€é¡¯ç¤º
                        # å¦‚æœæœ‰åŒ¹é…åˆ°ï¼Œé¡¯ç¤ºæ˜ç´°åç¨±ï¼›å¦‚æœæ²’åŒ¹é…åˆ°ï¼Œé¡¯ç¤ºç©º
                        if details:
                            matched_display = " | ".join(details)
                            if len(matched_display) > 25: matched_display = matched_display[:25] + "..."
                            matched_display += f" (å…±{len(details)}ç­†)"
                        else:
                            matched_display = "(ç„¡åŒ¹é…æ˜ç´°)"

                        # 2. è™•ç†ã€ŒåŒ¹é…åˆ†æ•¸/æ¨¡å¼ã€é¡¯ç¤º
                        if mode == "B":
                            score_display = "Mode B ğŸš€"
                        elif mode == "AB":
                            score_display = "Mode A+B"
                        elif mode == "A":
                            score_display = "Mode A" # A æ¨¡å¼é€šå¸¸æ˜¯ç´”é‹ç®—ï¼Œæ²’ç‰¹åˆ¥å­˜åˆ†æ•¸ï¼Œä½†èƒ½åŒ¹é…åˆ°å°±æ˜¯æœ‰åˆ†
                        else:
                            score_display = "-"

                        # 3. å¦‚æœæ˜¯ B æ¨¡å¼ï¼ŒæŠŠç†ç”±åŠ é€²èªªæ˜
                        final_note = ""
                        if note: final_note = f"[{note}] "
                        
                        # æª¢æŸ¥æ˜¯å¦æœ‰ç•°å¸¸æ¸…å–®è£¡çš„éŒ¯èª¤è¨Šæ¯ (é€™æ˜¯æœ€æº–çš„ç•°å¸¸ç†ç”±ä¾†æº)
                        err_obj = next((i for i in visible_issues 
                                        if "ç¸½è¡¨" in str(i.get('issue_type','')) and 
                                        (row.get('title','') in str(i.get('item','')))), None)
                        if err_obj:
                            final_note += err_obj['common_reason']

                        sum_data.append({
                            "ç‹€æ…‹": status,
                            "é ç¢¼": row.get('page', '?'),
                            "ç¸½è¡¨é …ç›®": row.get('title', ''),
                            "åˆ—è¡¨é …ç›®": matched_display,
                            "åŒ¹é…æ¨¡å¼": score_display,
                            "ç”³è«‹": row.get('apply_qty', 0),
                            "å¯¦äº¤": row.get('delivery_qty', row.get('target', 0)),
                            "èªªæ˜": final_note
                        })
                    
                    st.dataframe(
                        pd.DataFrame(sum_data), 
                        use_container_width=True, 
                        hide_index=True,
                        column_config={
                            "ç‹€æ…‹": st.column_config.TextColumn("ç‹€æ…‹", width="small"),
                            "ç¸½è¡¨é …ç›®": st.column_config.TextColumn("ç¸½è¡¨é …ç›®", width="medium"),
                            "åˆ—è¡¨é …ç›®": st.column_config.TextColumn("åˆ—è¡¨é …ç›® (å¯¦éš›é‹ç®—çµæœ)", width="medium", help="æœƒè¨ˆå¼•æ“å¯¦éš›ç´å…¥è¨ˆç®—çš„æ˜ç´°"),
                            "åŒ¹é…æ¨¡å¼": st.column_config.TextColumn("æ¨¡å¼", width="small"),
                            "èªªæ˜": st.column_config.TextColumn("ç•°å¸¸åŸå› ", width="large"),
                        }
                    )
                else:
                    st.info("æœ¬æ¬¡ç„¡ç¸½è¡¨æ•¸æ“šã€‚")

             # --- Tab 2: æ˜ç´°æª¢æŸ¥ (v5: èªæ„é˜²æ’ç‰ˆ) ---
            with tab_det:
                raw_det = cache.get("ai_extracted_data", [])
                
                if raw_det:
                    from thefuzz import fuzz

                    det_data = []
                    
                    # æ¨™æº–åŒ–å‡½å¼
                    def get_norm_key(page, title):
                        p_str = str(page).upper().replace("P.", "").replace(" ", "").strip()
                        t_str = str(title).upper().replace(" ", "").replace("\n", "").strip()
                        return p_str, t_str

                    # å®šç¾©ä»€éº¼æ˜¯ã€Œç¸½è¡¨é ã€çš„ä»£è™Ÿ
                    SUMMARY_PAGES = ["ç¸½è¡¨", "SUMMARY", "TOTAL", "0", "ALL", "å½™ç¸½"]

                    # 1. å»ºç«‹ç•°å¸¸è¨»å†Šè¡¨
                    issue_registry = []
                    current_issues = locals().get('visible_issues', [])
                    
                    for issue in current_issues:
                        ip, it = get_norm_key(issue.get('page', '?'), issue.get('item', ''))
                        
                        src = str(issue.get('source', ''))
                        itype = str(issue.get('issue_type', ''))
                        
                        flags = {"æœƒè¨ˆ": False, "å·¥ç¨‹": False, "æµç¨‹": False}
                        if "æµç¨‹" in src or "æº¯æº" in itype or "å·¥åº" in itype:
                            flags["æµç¨‹"] = True
                        elif "æœƒè¨ˆ" in src or "æ•¸é‡" in itype or "çµ±è¨ˆ" in itype or "ç¸½è¡¨" in itype:
                            flags["æœƒè¨ˆ"] = True
                        else:
                            flags["å·¥ç¨‹"] = True
                        
                        # æ¨™è¨˜é€™æ˜¯å¦ç‚ºä¸€å€‹ã€Œç¸½è¡¨ç´šã€çš„ç•°å¸¸
                        is_global_issue = (ip in SUMMARY_PAGES)
                        
                        issue_registry.append({
                            "p": ip, 
                            "t": it, 
                            "flags": flags, 
                            "is_global": is_global_issue
                        })

                    # 2. éæ­·æ‰€æœ‰æ˜ç´°é …ç›®
                    for row in raw_det:
                        rp, rt = get_norm_key(row.get('page', '?'), row.get('item_title', ''))
                        
                        # æ¨™è¨˜é€™è¡Œæ˜¯å¦çœ‹èµ·ä¾†åƒç¸½è¡¨æ¨™é¡Œ
                        row_is_summary_page = (rp in SUMMARY_PAGES)
                        
                        current_status = {"æœƒè¨ˆ": False, "å·¥ç¨‹": False, "æµç¨‹": False}
                        
                        for iss in issue_registry:
                            # æƒ…æ³ A: é ç¢¼å®Œå…¨ä¸€æ¨£
                            match_page = (rp == iss['p'])
                            
                            # æƒ…æ³ B: è·¨é é€šç·
                            cross_page_match = (iss['is_global'] or row_is_summary_page)
                            
                            if match_page or cross_page_match:
                                # æ¨™é¡Œæ¯”å°
                                threshold = 90 if cross_page_match else 85
                                score = fuzz.ratio(rt, iss['t'])
                                
                                if score > threshold:
                                    # ğŸ”¥ğŸ”¥ğŸ”¥ [æ–°å¢] èªæ„é˜²æ’æ©Ÿåˆ¶ (Semantic Guardrails) ğŸ”¥ğŸ”¥ğŸ”¥
                                    
                                    # Guard 1: æœ¬é«” vs è»¸é ¸ (çµ•å°äº’æ–¥)
                                    # é˜²æ­¢ "æœ¬é«”å†ç”Ÿ" æ’åˆ° "è»¸é ¸å†ç”Ÿ"
                                    has_body_iss = "æœ¬é«”" in iss['t']
                                    has_body_row = "æœ¬é«”" in rt
                                    has_journal_iss = any(k in iss['t'] for k in ["è»¸é ¸", "è»¸é ­", "è»¸ä½"])
                                    has_journal_row = any(k in rt for k in ["è»¸é ¸", "è»¸é ­", "è»¸ä½"])
                                    
                                    if (has_body_iss and has_journal_row) or (has_journal_iss and has_body_row):
                                        continue

                                    # Guard 2: å†ç”Ÿ vs æœªå†ç”Ÿ (çµ•å°äº’æ–¥)
                                    # é˜²æ­¢ "æœªå†ç”Ÿ" æ’åˆ° "å†ç”Ÿ" (å­—ä¸²åŒ…å«é—œä¿‚)
                                    is_unregen_iss = "æœªå†ç”Ÿ" in iss['t'] or "ç²—è»Š" in iss['t']
                                    is_unregen_row = "æœªå†ç”Ÿ" in rt or "ç²—è»Š" in rt
                                    
                                    # å¦‚æœä¸€å€‹æ˜¯æœªå†ç”Ÿï¼Œå¦ä¸€å€‹ä¸æ˜¯ï¼Œé‚£å°±çµ•å°ä¸æ˜¯åŒä¸€ä»¶äº‹
                                    if is_unregen_iss != is_unregen_row:
                                        continue
                                        
                                    # Guard 3: éŠ²è£œ (çµ•å°äº’æ–¥)
                                    # é˜²æ­¢ "è»Šä¿®" æ’åˆ° "éŠ²è£œ"
                                    weld_kws = ["éŠ²", "ç„Š", "é‰€"]
                                    is_weld_iss = any(k in iss['t'] for k in weld_kws)
                                    is_weld_row = any(k in rt for k in weld_kws)
                                    
                                    if is_weld_iss != is_weld_row:
                                        continue

                                    # --- é€šéæ‰€æœ‰é˜²æ’æª¢æŸ¥ï¼Œæ‰æ­£å¼äº®ç‡ˆ ---
                                    if iss['flags']['æœƒè¨ˆ']: current_status['æœƒè¨ˆ'] = True
                                    if iss['flags']['å·¥ç¨‹']: current_status['å·¥ç¨‹'] = True
                                    if iss['flags']['æµç¨‹']: current_status['æµç¨‹'] = True

                        # ç‡ˆè™Ÿè½‰æ›
                        light_eng = "ğŸ”´" if current_status["å·¥ç¨‹"] else "ğŸŸ¢"
                        light_acc = "ğŸ”´" if current_status["æœƒè¨ˆ"] else "ğŸŸ¢"
                        light_proc = "ğŸ”´" if current_status["æµç¨‹"] else "ğŸŸ¢"
                        
                        det_data.append({
                            "å·¥ç¨‹": light_eng,
                            "æœƒè¨ˆ": light_acc,
                            "æµç¨‹": light_proc,
                            "é ç¢¼": row.get('page', '?'),
                            "é …ç›®åç¨±": row.get('item_title', ''),
                            "åˆ†é¡åˆ¤å®š": row.get('category', ''),
                            "ç›®æ¨™": row.get('item_pc_target', 0),
                            "è¦æ ¼": (str(row.get('std_spec', ''))[:15] + '...') if row.get('std_spec') else ''
                        })
                    
                    df_det = pd.DataFrame(det_data)
                    
                    st.dataframe(
                        df_det, 
                        use_container_width=True, 
                        hide_index=True,
                        column_config={
                            "å·¥ç¨‹": st.column_config.TextColumn("å·¥ç¨‹", width="small", help="è¦æ ¼/åˆ†é¡æª¢æŸ¥"),
                            "æœƒè¨ˆ": st.column_config.TextColumn("æœƒè¨ˆ", width="small", help="æ•¸é‡/ç¸½è¡¨æª¢æŸ¥"),
                            "æµç¨‹": st.column_config.TextColumn("æµç¨‹", width="small", help="å·¥åº/æº¯æºæª¢æŸ¥"),
                            "åˆ†é¡åˆ¤å®š": st.column_config.TextColumn("Pythonåˆ†é¡"),
                        }
                    )
                else:
                    st.info("æœ¬æ¬¡ç„¡æ˜ç´°æ•¸æ“šã€‚")
            
        # 5. å¡ç‰‡å¾ªç’°é¡¯ç¤º (ä½¿ç”¨éæ¿¾å¾Œçš„ visible_issues)
        for item in visible_issues:
            # é€™è£¡å› ç‚º visible_issues å·²ç¶“æ¿¾æ‰ HIDDEN_DATA äº†ï¼Œæ‰€ä»¥ä¸éœ€è¦å†å¯« if continue
            with st.container(border=True):
                c1, c2 = st.columns([3, 1])
                source_label = item.get('source', '')
                issue_type = item.get('issue_type', 'ç•°å¸¸')
                
                # é ç¢¼è™•ç†
                page_str = item.get('page', '?')
                page_display = f"Pages: {page_str}" if "," in str(page_str) else f"P.{page_str}"

                c1.markdown(f"**{page_display} | {item.get('item')}** `{source_label}`")
                
                # ç‡ˆè™Ÿé‚è¼¯
                if any(kw in issue_type for kw in ["çµ±è¨ˆ", "æ•¸é‡", "æµç¨‹", "æº¯æº", "ç¸½è¡¨", "åŒ¯ç¸½", "ğŸš¨", "ğŸ›‘"]):
                    c2.error(f"{issue_type}")
                else:
                    c2.warning(f"{issue_type}")
                
                st.caption(f"åŸå› : {item.get('common_reason', '')}")
                
                failures = item.get('failures', [])
                if failures:
                    df = pd.DataFrame(failures)
                    rename_map = {"id": "ç·¨è™Ÿ", "val": "å¯¦æ¸¬", "target": "ç›®æ¨™", "calc": "ç‹€æ…‹", "note": "å‚™è¨»"}
                    df.rename(columns=rename_map, inplace=True)
                    
                    styler = df.style.set_properties(**{'text-align': 'center', 'white-space': 'nowrap'})
                    styler.set_table_styles([dict(selector='th', props=[('text-align', 'center')])])

                    # é‡å°æ–‡å­—è¼ƒé•·çš„æ¬„ä½é å·¦
                    left_cols = [c for c in ["é …ç›®åç¨±", "ç·¨è™Ÿ", "Item"] if c in df.columns]
                    if left_cols:
                        styler.set_properties(subset=left_cols, **{'text-align': 'left'})

                    # æ•¸å€¼æ ¼å¼åŒ–
                    def smart_fmt(x):
                        try:
                            f = float(x)
                            return f"{int(f)}" if abs(f - round(f)) < 1e-6 else f"{f:.2f}"
                        except: return str(x)

                    target_num_cols = [c for c in ["å¯¦æ¸¬", "ç›®æ¨™", "æ•¸é‡"] if c in df.columns]
                    if target_num_cols:
                        styler.format(smart_fmt, subset=target_num_cols)

                    st.dataframe(styler, use_container_width=True, hide_index=True)

            st.divider()
        
        # ä¸‹è¼‰æŒ‰éˆ•é‚è¼¯
        current_job_no = cache.get('job_no', 'Unknown')
        safe_job_no = str(current_job_no).replace("/", "_").replace("\\", "_").strip()
        file_name_str = f"{safe_job_no}_cleaned.json"

        # æº–å‚™åŒ¯å‡ºè³‡æ–™
        export_data = []
        for item in st.session_state.photo_gallery:
            export_data.append({
                "table_md": item.get('table_md'),
                "header_text": item.get('header_text'),
                "full_text": item.get('full_text'),
                "raw_json": item.get('raw_json')
            })
        json_str = json.dumps(export_data, indent=2, ensure_ascii=False)

        st.subheader("ğŸ’¾ æ¸¬è©¦è³‡æ–™å­˜æª”")
        st.caption(f"å·²è­˜åˆ¥å·¥ä»¤ï¼š**{current_job_no}**ã€‚ä¸‹è¼‰å¾Œå¯ä¾›ä¸‹æ¬¡æ¸¬è©¦ä½¿ç”¨ã€‚")
        
        st.download_button(
            label=f"â¬‡ï¸ ä¸‹è¼‰æ¸¬è©¦è³‡æ–™ ({file_name_str})",
            data=json_str,
            file_name=file_name_str,
            mime="application/json",
            type="primary"
        )

        with st.expander("ğŸ‘€ æŸ¥çœ‹å‚³çµ¦ AI çš„æœ€çµ‚æ–‡å­— (Prompt Input)"):
            st.caption("é€™æ‰æ˜¯ AI çœŸæ­£è®€åˆ°çš„å…§å®¹ (å·²éæ¿¾é›œè¨Š)ï¼š")
            st.code(cache.get('combined_input', 'ç„¡è³‡æ–™'), language='markdown')
    
    if st.session_state.photo_gallery and st.session_state.get('source_mode') != 'json':
        st.caption("å·²æ‹æ”ç…§ç‰‡ï¼š")
        cols = st.columns(4)
        for idx, item in enumerate(st.session_state.photo_gallery):
            with cols[idx % 4]:
                if item.get('file'):
                    
                    # ğŸ”¥ ä¿®æ”¹é€™æ®µï¼šåˆ¤æ–·æ˜¯ PDF é‚„æ˜¯åœ–ç‰‡
                    if item['file'].type == "application/pdf":
                        # å¦‚æœæ˜¯ PDFï¼Œé¡¯ç¤ºä¸€å€‹æ–‡ä»¶åœ–ç¤ºï¼Œä¸è¦ç”¨ st.image
                        st.markdown(f"ğŸ“„ **PDF æ–‡ä»¶**\n\n{item['file'].name}")
                    else:
                        # å¦‚æœæ˜¯åœ–ç‰‡ï¼Œç…§å¸¸é¡¯ç¤º
                        st.image(item['file'], caption=f"P.{idx+1}", use_container_width=True)
                
                if st.button("âŒ", key=f"del_{idx}"):
                    st.session_state.photo_gallery.pop(idx)
                    st.session_state.analysis_result_cache = None
                    st.rerun()
else:
    st.info("ğŸ‘† è«‹é»æ“Šä¸Šæ–¹æŒ‰éˆ•é–‹å§‹æ–°å¢ç…§ç‰‡")
